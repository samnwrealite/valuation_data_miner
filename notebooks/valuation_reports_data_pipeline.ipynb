{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "773f0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Docling imports\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "# LLM imports\n",
    "import ollama\n",
    "\n",
    "# Supabase\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Validation\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "print(\" All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd92b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration loaded\n",
      "  PDF Directory: C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n",
      "  Output Directory: ./extracted_data\n",
      "  LLM Model: mistral\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "\n",
    "with open(dotenv_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        key, value = line.strip().split(\"=\", 1)\n",
    "        os.environ[key] = value  # set environment variable\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "# Supabase credentials\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "SUPABASE_TABLE = \"property_valuations\"\n",
    "\n",
    "\n",
    "\n",
    "# LLM settings\n",
    "LLM_MODEL = \"mistral\"\n",
    "LLM_TEMPERATURE = 0.1\n",
    "\n",
    "# Processing settings\n",
    "BATCH_SIZE = 5\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\" Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  LLM Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "448357e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"property_id\", \"valuation_report\", \"property_details\", \"valuations\"],\n",
    "    \"properties\": {\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"valuation_report\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"report_reference\": {\"type\": \"string\"},\n",
    "                \"valuer\": {\"type\": \"string\"},\n",
    "                \"valuers\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"string\"},\n",
    "                            \"qualification\": {\"type\": \"string\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inspection_date\": {\"type\": \"string\"},\n",
    "                \"report_date\": {\"type\": \"string\"},\n",
    "                \"client\": {\"type\": \"string\"},\n",
    "                \"client_address\": {\"type\": \"string\"},\n",
    "                \"purpose\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_details\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"apartment_number\": {\"type\": \"string\"},\n",
    "                \"block\": {\"type\": \"string\"},\n",
    "                \"floor\": {\"type\": \"string\"},\n",
    "                \"title_details\": {\"type\": \"object\"},\n",
    "                \"location\": {\"type\": \"object\"},\n",
    "                \"tenure\": {\"type\": \"object\"},\n",
    "                \"registered_proprietors\": {\"type\": \"array\"},\n",
    "                \"ownership_type\": {\"type\": \"string\"},\n",
    "                \"encumbrances\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_description\": {\"type\": \"object\"},\n",
    "        \"apartment_details\": {\"type\": \"object\"},\n",
    "        \"occupancy\": {\"type\": \"string\"},\n",
    "        \"condition\": {\"type\": \"string\"},\n",
    "        \"market_assessment\": {\"type\": \"object\"},\n",
    "        \"valuations\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"current_market_value\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"amount\": {\"type\": \"number\"},\n",
    "                        \"currency\": {\"type\": \"string\"},\n",
    "                        \"amount_words\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"valuation_methodology\": {\"type\": \"array\"},\n",
    "        \"lease_details\": {\"type\": \"object\"},\n",
    "        \"compliance\": {\"type\": \"object\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"JSON schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a754d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase client initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "\n",
    "with open(dotenv_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        key, value = line.strip().split(\"=\", 1)\n",
    "        os.environ[key] = value  # set environment variable\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client using environment variables\"\"\"\n",
    "    try:\n",
    "        supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "        supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "        if not supabase_url or not supabase_key:\n",
    "            print(\"Supabase credentials not configured\")\n",
    "            return None\n",
    "\n",
    "        client = create_client(supabase_url, supabase_key)\n",
    "        print(\"Supabase client initialized\")\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to initialize Supabase: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "840c4e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PDF processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_docling(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a PDF file using Docling with OCR capabilities\n",
    "    Returns extracted text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure pipeline with OCR\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.do_ocr = True\n",
    "        pipeline_options.do_table_structure = True\n",
    "        \n",
    "        # Initialize converter\n",
    "        converter = DocumentConverter(\n",
    "            allowed_formats=[InputFormat.PDF],\n",
    "            format_options={\n",
    "                InputFormat.PDF: pipeline_options\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Convert document\n",
    "        result = converter.convert(pdf_path)\n",
    "        \n",
    "        # Extract text and tables\n",
    "        full_text = result.document.export_to_markdown()\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Docling processing failed: {str(e)}\")\n",
    "\n",
    "print(\" PDF processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1833f1",
   "metadata": {},
   "source": [
    "###  LLM Extraction with Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70e86260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_with_llm(text_content: str, filename: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract structured data from text using local Mistral model\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a property valuation data extraction expert. \n",
    "Extract information from property valuation reports and return ONLY valid JSON.\n",
    "Follow the exact schema provided. If information is not found, use empty strings or empty objects.\n",
    "Do not include any markdown formatting, explanations, or text outside the JSON object.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract property valuation data from this document and return as JSON following this schema:\n",
    "\n",
    "{json.dumps(VALUATION_SCHEMA, indent=2)}\n",
    "\n",
    "Document content:\n",
    "{text_content[:10000]}\n",
    "\n",
    "Return ONLY the JSON object, no additional text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": LLM_TEMPERATURE,\n",
    "                \"num_predict\": 4096\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        response_text = response['message']['content']\n",
    "        \n",
    "        # Clean response (remove markdown if present)\n",
    "        response_text = response_text.strip()\n",
    "        if response_text.startswith('```json'):\n",
    "            response_text = response_text[7:]\n",
    "        if response_text.startswith('```'):\n",
    "            response_text = response_text[3:]\n",
    "        if response_text.endswith('```'):\n",
    "            response_text = response_text[:-3]\n",
    "        response_text = response_text.strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        extracted_data = json.loads(response_text)\n",
    "        \n",
    "        # Add metadata\n",
    "        if not extracted_data.get('property_id'):\n",
    "            extracted_data['property_id'] = Path(filename).stem\n",
    "        \n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        raise Exception(f\"Failed to parse JSON from LLM response: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"LLM extraction failed: {str(e)}\")\n",
    "\n",
    "print(\"LLM extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611cb5d",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ecd342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Validate extracted data against schema\n",
    "    Returns (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\" Validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992b889",
   "metadata": {},
   "source": [
    "### Supabase upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e9b96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase upload function defined\n"
     ]
    }
   ],
   "source": [
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload extracted data to Supabase\n",
    "    Returns True if successful\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Add metadata\n",
    "        upload_data = data.copy()\n",
    "        upload_data['source_filename'] = filename\n",
    "        upload_data['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Insert into Supabase\n",
    "        response = client.table(SUPABASE_TABLE).insert(upload_data).execute()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n",
    "\n",
    "print(\"Supabase upload function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a98331",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94c93479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Main pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf(pdf_path: str, filename: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single PDF through the entire pipeline\n",
    "    Returns result dictionary with status\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Docling OCR\n",
    "        result['stage'] = 'docling'\n",
    "        text_content = process_pdf_with_docling(pdf_path)\n",
    "        \n",
    "        # Stage 2: LLM Extraction\n",
    "        result['stage'] = 'extraction'\n",
    "        extracted_data = extract_with_llm(text_content, filename)\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        \n",
    "        if not is_valid:\n",
    "            result['error'] = f\"Validation failed: {validation_error}\"\n",
    "        \n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        # Stage 4: Save locally\n",
    "        result['stage'] = 'saving'\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Stage 5: Upload to Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'upload'\n",
    "            upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['stage'] = 'complete'\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        \n",
    "        # Log error\n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\n\")\n",
    "            f.write(f\"Error: {result['error']}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\" Main pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad814a2",
   "metadata": {},
   "source": [
    "### Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59e32a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs():\n",
    "    \"\"\"\n",
    "    Process all PDFs in the configured directory\n",
    "    \"\"\"\n",
    "    # Get all PDF files\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"✗ No PDF files found in {PDF_DIR}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each PDF\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        print(f\"\\n[{pdf_file.name}]\")\n",
    "        result = process_single_pdf(str(pdf_file), pdf_file.name)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ✓ Successfully processed\")\n",
    "        else:\n",
    "            print(f\"  ✗ Failed at stage '{result['stage']}': {result['error']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"Total PDFs: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"\\nOutput saved to: {OUTPUT_DIR}\")\n",
    "    if failed > 0:\n",
    "        print(f\"Error logs saved to: {ERROR_LOG_DIR}\")\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"processing_summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total': len(results),\n",
    "            'successful': successful,\n",
    "            'failed': failed,\n",
    "            'results': results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Batch processing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853339f",
   "metadata": {},
   "source": [
    "### Ollama setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec296810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:13:28,711 - INFO - HTTP Request: GET http://localhost:11434/api/tags \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW MODEL RESPONSE: models=[Model(model='mistral:latest', modified_at=datetime.datetime(2025, 11, 23, 8, 42, 42, 366963, tzinfo=TzInfo(10800)), digest='6577803aa9a036369e481d648a2baebb381ebc6e897f2bb9a766a2aa7bfbc1cf', size=4372824384, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.2B', quantization_level='Q4_K_M'))]\n",
      "Detected models: ['mistral:latest']\n",
      "✓ Model 'mistral' is available\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "try:\n",
    "    response = ollama.list()\n",
    "    print(\"RAW MODEL RESPONSE:\", response)\n",
    "\n",
    "    models = response.models  # this is a list of Model objects\n",
    "    model_names = [m.model for m in models]  # use .model attribute\n",
    "\n",
    "    print(\"Detected models:\", model_names)\n",
    "\n",
    "    if LLM_MODEL not in model_names and f\"{LLM_MODEL}:latest\" not in model_names:\n",
    "        print(f\"⚠ Warning: Model '{LLM_MODEL}' not found in Ollama\")\n",
    "        print(f\"Available models: {model_names}\")\n",
    "        print(\"\\nTo install Mistral, run in terminal: ollama pull mistral\")\n",
    "    else:\n",
    "        print(f\"✓ Model '{LLM_MODEL}' is available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not check Ollama models: {e}\")\n",
    "    print(\"Make sure Ollama is running (run 'ollama serve' in terminal')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b684e",
   "metadata": {},
   "source": [
    "### View sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beaf38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No output files found yet\n"
     ]
    }
   ],
   "source": [
    "json_files = list(Path(OUTPUT_DIR).glob(\"*.json\"))\n",
    "json_files = [f for f in json_files if f.name != \"processing_summary.json\"]\n",
    "\n",
    "if json_files:\n",
    "    sample_file = json_files[0]\n",
    "    print(f\"Sample output from: {sample_file.name}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(sample_file, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(json.dumps(sample_data, indent=2))\n",
    "else:\n",
    "    print(\"No output files found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd38f",
   "metadata": {},
   "source": [
    "### reprocess failed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6aeac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess_failed(previous_results):\n",
    "    \"\"\"Reprocess only the failed PDFs\"\"\"\n",
    "    failed_files = [r['filename'] for r in previous_results if not r['success']]\n",
    "    \n",
    "    if not failed_files:\n",
    "        print(\"No failed files to reprocess\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Reprocessing {len(failed_files)} failed PDFs...\")\n",
    "    \n",
    "    retry_results = []\n",
    "    for filename in tqdm(failed_files, desc=\"Retrying\"):\n",
    "        pdf_path = os.path.join(PDF_DIR, filename)\n",
    "        result = process_single_pdf(pdf_path, filename)\n",
    "        retry_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ✓ {filename} - Success on retry\")\n",
    "        else:\n",
    "            print(f\"  ✗ {filename} - Failed again\")\n",
    "    \n",
    "    return retry_results\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    retry_results = reprocess_failed(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c39a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:13:30,856 - INFO - HTTP Request: GET https://vtsjmfrfvmqrcjheubyn.supabase.co/rest/v1/property_valuations?select=%2A&limit=5 \"HTTP/2 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying Supabase: {'message': \"Could not find the table 'public.property_valuations' in the schema cache\", 'code': 'PGRST205', 'hint': None, 'details': None}\n"
     ]
    }
   ],
   "source": [
    "def query_supabase_sample():\n",
    "    \"\"\"Query and display sample data from Supabase\"\"\"\n",
    "    if supabase_client is None:\n",
    "        print(\"Supabase client not initialized\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = supabase_client.table(SUPABASE_TABLE).select(\"*\").limit(5).execute()\n",
    "        print(f\"Sample records from Supabase ({SUPABASE_TABLE}):\\n\")\n",
    "        print(json.dumps(response.data, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Supabase: {e}\")\n",
    "\n",
    "query_supabase_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20e43462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze processing results\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Stage failures\n",
    "    stage_failures = {}\n",
    "    for r in results:\n",
    "        if not r['success'] and r['stage']:\n",
    "            stage_failures[r['stage']] = stage_failures.get(r['stage'], 0) + 1\n",
    "    \n",
    "    if stage_failures:\n",
    "        print(\"Failures by stage:\")\n",
    "        for stage, count in sorted(stage_failures.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {stage}: {count}\")\n",
    "    \n",
    "    # Validation stats\n",
    "    validation_issues = [r for r in results if r['error'] and 'Validation' in r['error']]\n",
    "    print(f\"\\nValidation issues: {len(validation_issues)}\")\n",
    "    \n",
    "    # Success rate\n",
    "    success_rate = (sum(1 for r in results if r['success']) / len(results)) * 100\n",
    "    print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    analyze_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c05b1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv():\n",
    "    \"\"\"Export extracted valuations to CSV for easy viewing\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    json_files = list(Path(OUTPUT_DIR).glob(\"*.json\"))\n",
    "    json_files = [f for f in json_files if f.name != \"processing_summary.json\"]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files to export\")\n",
    "        return\n",
    "    \n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"valuations_summary.csv\")\n",
    "    \n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['property_id', 'client', 'location', 'valuation_amount', \n",
    "                     'currency', 'report_date', 'source_file']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            row = {\n",
    "                'property_id': data.get('property_id', ''),\n",
    "                'client': data.get('valuation_report', {}).get('client', ''),\n",
    "                'location': data.get('property_details', {}).get('location', {}).get('area', ''),\n",
    "                'valuation_amount': data.get('valuations', {}).get('current_market_value', {}).get('amount', ''),\n",
    "                'currency': data.get('valuations', {}).get('current_market_value', {}).get('currency', ''),\n",
    "                'report_date': data.get('valuation_report', {}).get('report_date', ''),\n",
    "                'source_file': json_file.name\n",
    "            }\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"✓ CSV exported to: {csv_path}\")\n",
    "\n",
    "# Uncomment to export to CSV:\n",
    "# export_to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

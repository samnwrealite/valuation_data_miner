{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: Z:\\Risper M\\2025 valuation reports\\Year 2025\n",
      "  Gemini Model: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"Z:\\Risper M\\2025 valuation reports\\Year 2025\"\n",
    "OUTPUT_DIR = \"./additional_2025v1\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "# print(f\"Key: {SUPABASE_KEY}, \\n URL: {SUPABASE_URL}\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"property_id\",\n",
    "        \"report_reference\",\n",
    "        \"market_value_amount\",\n",
    "        \"metadata\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "\n",
    "        # --- CORE IDENTIFIERS ---\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"report_reference\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- TITLE INFORMATION ---\n",
    "        \"title_number\": {\"type\": \"string\"},\n",
    "        \"lr_number\": {\"type\": \"string\"},\n",
    "        \"ir_number\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- CLIENT + VALUER ---\n",
    "        \"client_name\": {\"type\": \"string\"},\n",
    "        \"valuer_name\": {\"type\": \"string\"},\n",
    "        \"inspection_date\": {\"type\": \"string\"},\n",
    "        \"valuation_date\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- LOCATION ---\n",
    "        \"location_county\": {\"type\": \"string\"},\n",
    "        \"location_description\": {\"type\": \"string\"},\n",
    "        \"location_coordinates\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- PARCEL DETAILS ---\n",
    "        \"plot_area_hectares\": {\"type\": \"number\"},\n",
    "        \"plot_area_acres\": {\"type\": \"number\"},\n",
    "        \"land_use\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- NEW LAND PHYSICAL FEATURES ---\n",
    "        \"plot_shape\": {\"type\": \"string\"},     # rectangular, irregular, square\n",
    "        \"soil_type\": {\"type\": \"string\"},      # black cotton, red soil, sandy, loam, murram\n",
    "        \"gradient\": {\"type\": \"string\"},       # flat, gentle slope, moderate slope, steep\n",
    "        \"drainage\": {\"type\": \"string\"},       # optional recommendation\n",
    "        \"vegetation\": {\"type\": \"string\"},     # optional recommendation\n",
    "\n",
    "        # --- TENURE + OWNERSHIP ---\n",
    "        \"tenure_type\": {\"type\": \"string\"},\n",
    "        \"registered_proprietor\": {\"type\": \"string\"},\n",
    "        \"ownership_type\": {\"type\": \"string\"},\n",
    "        \"encumbrances\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- VALUATION ---\n",
    "        \"market_value_amount\": {\"type\": \"number\"},\n",
    "        \"market_value_currency\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- METADATA ---\n",
    "        \"metadata\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"source_file\": {\"type\": \"string\"},\n",
    "                \"file_size_kb\": {\"type\": \"number\"},\n",
    "                \"processing_time_seconds\": {\"type\": \"number\"},\n",
    "                \"ocr_used\": {\"type\": \"boolean\"},\n",
    "                \"pages_processed\": {\"type\": \"number\"},\n",
    "                \"model_name\": {\"type\": \"string\"},\n",
    "                \"timestamp\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Updated JSON schema defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_TABLE = \"nrb_2025\"   \n",
    "\n",
    "\n",
    "def sanitize_dates_for_supabase(data: dict) -> dict:\n",
    "    \"\"\"Convert empty string dates to None for Supabase insert\"\"\"\n",
    "    date_fields = [\"inspection_date\", \"valuation_date\"]\n",
    "    for field in date_fields:\n",
    "        if field in data and (data[field] == \"\" or data[field] is None):\n",
    "            data[field] = None\n",
    "    return data\n",
    "\n",
    "\n",
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload extracted valuation data to Supabase (restricted to valid columns).\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = sanitize_dates_for_supabase(data.copy())\n",
    "\n",
    "        # Allowed columns EXACTLY as in your Supabase table\n",
    "        allowed_columns = {\n",
    "            \"property_id\",\n",
    "            \"report_reference\",\n",
    "            \"title_number\",\n",
    "            \"lr_number\",\n",
    "            \"ir_number\",\n",
    "            \"client_name\",\n",
    "            \"valuer_name\",\n",
    "            \"inspection_date\",\n",
    "            \"valuation_date\",\n",
    "            \"location_county\",\n",
    "            \"location_description\",\n",
    "            \"location_coordinates\",\n",
    "            \"plot_area_hectares\",\n",
    "            \"plot_area_acres\",\n",
    "            \"land_use\",\n",
    "            \"plot_shape\",\n",
    "            \"soil_type\",\n",
    "            \"gradient\",\n",
    "            \"drainage\",\n",
    "            \"vegetation\",\n",
    "            \"tenure_type\",\n",
    "            \"registered_proprietor\",\n",
    "            \"ownership_type\",\n",
    "            \"encumbrances\",\n",
    "            \"market_value_amount\",\n",
    "            \"market_value_currency\",\n",
    "            \"metadata\"\n",
    "        }\n",
    "\n",
    "        # Remove any field not in table\n",
    "        clean_record = {k: v for k, v in upload_data.items() if k in allowed_columns}\n",
    "\n",
    "        # Insert into your table\n",
    "        client.table(SUPABASE_TABLE).insert(clean_record).execute()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str, dpi: int = 150) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR safely (memory-friendly).\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        import gc\n",
    "\n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images at {dpi} dpi...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, fmt='jpeg', thread_count=2)\n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            # OCR the page (use psm 6 for memory efficiency)\n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 6'\n",
    "            ).strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "            \n",
    "            # Free memory after each page\n",
    "            image.close()\n",
    "            del image\n",
    "            gc.collect()\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output for debugging\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Updated OCR function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "from time import sleep\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def clean_text_for_model(text: str) -> str:\n",
    "    \"\"\"Remove page headers, footers, and empty lines\"\"\"\n",
    "    text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_report_reference(text: str) -> str:\n",
    "    \"\"\"Detect report reference numbers\"\"\"\n",
    "    pattern = r\"\\b(?:[A-Z]{2,3}/)?(?:[A-Z]{2,3}/)?\\d{3,5}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        return max(matches, key=len)\n",
    "    return \"\"\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}')\n",
    "    if start != -1 and end != -1:\n",
    "        response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Merge dictionaries recursively\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# CRITICAL: Configure safety settings\n",
    "# -----------------------------\n",
    "def get_safe_generation_config():\n",
    "    \"\"\"Returns generation config with safety settings disabled\"\"\"\n",
    "    safety_settings = [\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=4096,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    return generation_config, safety_settings\n",
    "\n",
    "# -----------------------------\n",
    "# Rate limiting with retry logic\n",
    "# -----------------------------\n",
    "def call_gemini_with_retry(model, prompt, generation_config, safety_settings, max_retries=3):\n",
    "    \"\"\"\n",
    "    Call Gemini API with automatic retry on rate limit errors.\n",
    "    Implements exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            # Check if response was blocked\n",
    "            if response.prompt_feedback.block_reason:\n",
    "                print(f\"      âš  Response blocked: {response.prompt_feedback.block_reason}\")\n",
    "                return None\n",
    "            \n",
    "            # Check if we have valid text\n",
    "            if hasattr(response, 'text'):\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"      âš  No text in response\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            \n",
    "            # Check if it's a rate limit error (429)\n",
    "            if \"429\" in error_str or \"quota\" in error_str.lower():\n",
    "                # Extract retry delay from error message\n",
    "                import re\n",
    "                retry_match = re.search(r'retry in (\\d+\\.\\d+)s', error_str)\n",
    "                \n",
    "                if retry_match:\n",
    "                    retry_seconds = float(retry_match.group(1))\n",
    "                else:\n",
    "                    # Default exponential backoff: 5s, 15s, 30s\n",
    "                    retry_seconds = 5 * (3 ** attempt)\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"      âš  Rate limit hit. Waiting {retry_seconds:.1f}s before retry {attempt+1}/{max_retries}...\")\n",
    "                    sleep(retry_seconds)\n",
    "                else:\n",
    "                    print(f\"      âœ— Rate limit exceeded after {max_retries} attempts\")\n",
    "                    raise e\n",
    "            else:\n",
    "                # Non-rate-limit error, raise immediately\n",
    "                raise e\n",
    "    \n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction function with rate limiting\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract structured property valuation data from a Kenyan PDF OCR text.\n",
    "    Two-pass approach with automatic rate limiting.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined_result = {}\n",
    "    cleaned_text = clean_text_for_model(text_content)\n",
    "    report_ref_guess = find_report_reference(cleaned_text[:3000])\n",
    "    \n",
    "    # Get safe configuration\n",
    "    generation_config, safety_settings = get_safe_generation_config()\n",
    "    \n",
    "    # Initialize model with safety settings\n",
    "    # Using gemini-2.0-flash instead of 2.0 for better rate limits\n",
    "    model = genai.GenerativeModel(\n",
    "        \"models/gemini-2.0-flash\",  # Changed to 1.5 for better free tier limits\n",
    "        safety_settings=safety_settings\n",
    "    )\n",
    "    \n",
    "    # -----------------------------\n",
    "    # PASS 1: Core identifiers\n",
    "    # -----------------------------\n",
    "    prompt_ids = f\"\"\"\n",
    "You are performing a DOCUMENT TRANSCRIPTION task.\n",
    "Extract structured data EXACTLY as it appears in the source document.\n",
    "\n",
    "Extract these fields from the Kenyan valuation report:\n",
    "- report_reference (format: SOO/DOO/5310/1/25)\n",
    "- title_number\n",
    "- lr_number\n",
    "- ir_number\n",
    "- client_name\n",
    "- valuer_name\n",
    "- inspection_date (YYYY/MM/DD format)\n",
    "- valuation_date (YYYY/MM/DD format)\n",
    "\n",
    "Hint for report_reference: \"{report_ref_guess}\"\n",
    "\n",
    "Return ONLY this JSON structure:\n",
    "{{\n",
    "  \"property_id\": \"\",\n",
    "  \"report_reference\": \"\",\n",
    "  \"title_number\": \"\",\n",
    "  \"lr_number\": \"\",\n",
    "  \"ir_number\": \"\",\n",
    "  \"client_name\": \"\",\n",
    "  \"valuer_name\": \"\",\n",
    "  \"inspection_date\": \"\",\n",
    "  \"valuation_date\": \"\"\n",
    "}}\n",
    "\n",
    "SOURCE TEXT (first 5000 chars):\n",
    "{cleaned_text[:5000]}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"      â†’ Extracting IDs...\")\n",
    "        response = call_gemini_with_retry(model, prompt_ids, generation_config, safety_settings)\n",
    "        \n",
    "        if response:\n",
    "            ids_json = clean_gemini_json(response.text)\n",
    "            extracted_ids = json.loads(ids_json)\n",
    "            combined_result = deep_merge(combined_result, extracted_ids)\n",
    "            print(\"      âœ“ IDs extracted\")\n",
    "        else:\n",
    "            print(\"      âš  ID extraction failed\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"      âš  JSON parsing error in IDs: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      âš  Gemini ID extraction failed: {e}\")\n",
    "    \n",
    "    # Add delay between API calls to respect rate limits\n",
    "    sleep(2)  # 2 second delay between passes\n",
    "    \n",
    "    # -----------------------------\n",
    "    # PASS 2: Land, valuation, metadata\n",
    "    # -----------------------------\n",
    "    prompt_land = f\"\"\"\n",
    "You are performing a DOCUMENT TRANSCRIPTION task.\n",
    "Extract structured data EXACTLY as it appears in the source document.\n",
    "\n",
    "Extract these fields from the Kenyan valuation report:\n",
    "- location_county\n",
    "- location_description\n",
    "- location_coordinates\n",
    "- plot_area_hectares (number)\n",
    "- plot_area_acres (number)\n",
    "- land_use\n",
    "- plot_shape\n",
    "- soil_type\n",
    "- gradient\n",
    "- tenure_type\n",
    "- registered_proprietor\n",
    "- encumbrances\n",
    "- market_value_amount (number)\n",
    "- market_value_currency\n",
    "\n",
    "Return ONLY this JSON structure:\n",
    "{{\n",
    "  \"location_county\": \"\",\n",
    "  \"location_description\": \"\",\n",
    "  \"location_coordinates\": \"\",\n",
    "  \"plot_area_hectares\": 0,\n",
    "  \"plot_area_acres\": 0,\n",
    "  \"land_use\": \"\",\n",
    "  \"plot_shape\": \"\",\n",
    "  \"soil_type\": \"\",\n",
    "  \"gradient\": \"\",\n",
    "  \"drainage\": \"\",\n",
    "  \"vegetation\": \"\",\n",
    "  \"tenure_type\": \"\",\n",
    "  \"registered_proprietor\": \"\",\n",
    "  \"ownership_type\": \"\",\n",
    "  \"encumbrances\": \"\",\n",
    "  \"market_value_amount\": 0,\n",
    "  \"market_value_currency\": \"KShs\"\n",
    "}}\n",
    "\n",
    "SOURCE TEXT:\n",
    "{cleaned_text[:15000]}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"      â†’ Extracting land/valuation data...\")\n",
    "        response = call_gemini_with_retry(model, prompt_land, generation_config, safety_settings)\n",
    "        \n",
    "        if response:\n",
    "            land_json = clean_gemini_json(response.text)\n",
    "            extracted_land = json.loads(land_json)\n",
    "            combined_result = deep_merge(combined_result, extracted_land)\n",
    "            print(\"      âœ“ Land data extracted\")\n",
    "        else:\n",
    "            print(\"      âš  Land extraction failed\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"      âš  JSON parsing error in Land: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      âš  Gemini Land extraction failed: {e}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Final adjustments\n",
    "    # -----------------------------\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "    if \"report_reference\" not in combined_result or not combined_result[\"report_reference\"]:\n",
    "        combined_result[\"report_reference\"] = report_ref_guess\n",
    "    \n",
    "    # Metadata skeleton\n",
    "    if \"metadata\" not in combined_result:\n",
    "        combined_result[\"metadata\"] = {\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": 0,\n",
    "            \"processing_time_seconds\": 0,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": 0,\n",
    "            \"model_name\": \"gemini-1.5-flash\",\n",
    "            \"timestamp\": \"\"\n",
    "        }\n",
    "    \n",
    "    # Ensure string fields\n",
    "    string_fields = [\n",
    "        \"title_number\", \"lr_number\", \"ir_number\", \"client_name\",\n",
    "        \"valuer_name\", \"inspection_date\", \"valuation_date\"\n",
    "    ]\n",
    "    \n",
    "    for field in string_fields:\n",
    "        if field in combined_result and combined_result[field] is None:\n",
    "            combined_result[field] = \"\"\n",
    "    \n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined with full metadata\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        pages_processed = text_content.count('--- Page')  # approximate page count\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Inject real metadata\n",
    "        # -------------------------\n",
    "        file_size_kb = round(Path(pdf_path).stat().st_size / 1024, 2)\n",
    "        processing_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        if \"metadata\" not in extracted_data:\n",
    "            extracted_data[\"metadata\"] = {}\n",
    "        \n",
    "        extracted_data[\"metadata\"].update({\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined with full metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in PDF_DIR, generate OCR, send to Gemini, save JSON, and upload to Supabase.\n",
    "       Skips PDFs that already have a JSON output file.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Scan directories\n",
    "    # -------------------------\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    # Identify JSON files already processed\n",
    "    processed_json_stems = {p.stem for p in Path(OUTPUT_DIR).glob(\"*.json\")}\n",
    "\n",
    "    # Filter: process ONLY new PDFs\n",
    "    pdf_files = [p for p in pdf_files if p.stem not in processed_json_stems]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"ðŸŽ‰ All PDFs already processed â€” nothing to do.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} NEW PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "    # Process each PDF\n",
    "    # -------------------------\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        file_start_time = time.time()\n",
    "        file_size_kb = round(pdf_path.stat().st_size / 1024, 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. OCR extraction\n",
    "        # -------------------------\n",
    "        text_content = process_pdf_with_fallback(str(pdf_path))\n",
    "        pages_processed = text_content.count('--- Page')  # reliable heuristic\n",
    "        print(f\"  âœ“ OCR complete ({pages_processed} pages, {len(text_content)} chars)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Gemini extraction\n",
    "        # -------------------------\n",
    "        extracted = extract_with_gemini(text_content, pdf_path.name)\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Add metadata to JSON\n",
    "        # -------------------------\n",
    "        processing_time = round(time.time() - file_start_time, 2)\n",
    "        if \"metadata\" not in extracted:\n",
    "            extracted[\"metadata\"] = {}\n",
    "\n",
    "        extracted[\"metadata\"].update({\n",
    "            \"source_file\": pdf_path.name,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Save the JSON output\n",
    "        # -------------------------\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{pdf_path.stem}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ“ Saved â†’ {output_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Upload to Supabase\n",
    "        # -------------------------\n",
    "        if supabase_client:\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted, pdf_path.name)\n",
    "                print(f\"âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Supabase upload failed: {str(e)[:120]}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Track results\n",
    "        # -------------------------\n",
    "        results.append({\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"success\": bool(extracted),\n",
    "            \"data\": extracted,\n",
    "            \"timing\": {\"processing_seconds\": processing_time},\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Save batch summary\n",
    "    # -------------------------\n",
    "    total_time = round(time.time() - total_start, 2)\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Batch finished in {total_time/60:.1f} minutes\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "179daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ PROCESSING 2 NEW PDFs\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "[1/2] doc42281920251030133205.pdf\n",
      "======================================================================\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images at 150 dpi...\n",
      "    â†’ Running OCR on 21 pages...\n",
      "      â†’ Page 1/21... âœ“ 968 chars\n",
      "      â†’ Page 2/21... âœ“ 2401 chars\n",
      "      â†’ Page 3/21... âœ“ 174 chars\n",
      "      â†’ Page 4/21... âœ“ 219 chars\n",
      "      â†’ Page 5/21... âœ“ 1014 chars\n",
      "      â†’ Page 6/21... âœ“ 1472 chars\n",
      "      â†’ Page 7/21... âœ“ 1528 chars\n",
      "      â†’ Page 8/21... âœ“ 1009 chars\n",
      "      â†’ Page 9/21... âœ“ 922 chars\n",
      "      â†’ Page 10/21... âœ“ 2244 chars\n",
      "      â†’ Page 11/21... âœ“ 1888 chars\n",
      "      â†’ Page 12/21... âœ“ 1226 chars\n",
      "      â†’ Page 13/21... âœ“ 1007 chars\n",
      "      â†’ Page 14/21... âœ“ 545 chars\n",
      "      â†’ Page 15/21... âœ“ 842 chars\n",
      "      â†’ Page 16/21... âœ“ 687 chars\n",
      "      â†’ Page 17/21... âœ“ 931 chars\n",
      "      â†’ Page 18/21... âœ“ 93 chars\n",
      "      â†’ Page 19/21... âœ“ 2059 chars\n",
      "      â†’ Page 20/21... âœ“ 2972 chars\n",
      "      â†’ Page 21/21... âœ“ 351 chars\n",
      "    â†’ OCR complete: 24919 characters\n",
      "  âœ“ OCR complete (21 pages, 24919 chars)\n",
      "      â†’ Extracting IDs...\n",
      "      âœ“ IDs extracted\n",
      "      â†’ Extracting land/valuation data...\n",
      "      âœ“ Land data extracted\n",
      "âœ“ Saved â†’ ./additional_2025v1\\doc42281920251030133205.json\n",
      "âš  Supabase upload failed: Supabase upload failed: {'message': \"Could not find the 'drainage' column of 'nrb_2025' in the schema cache\", 'code': 'P\n",
      "\n",
      "======================================================================\n",
      "[2/2] doc42282020251030133255.pdf\n",
      "======================================================================\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images at 150 dpi...\n",
      "    â†’ Running OCR on 14 pages...\n",
      "      â†’ Page 1/14... âœ“ 864 chars\n",
      "      â†’ Page 2/14... âœ“ 2366 chars\n",
      "      â†’ Page 3/14... âœ“ 185 chars\n",
      "      â†’ Page 4/14... âœ“ 730 chars\n",
      "      â†’ Page 5/14... âœ“ 1356 chars\n",
      "      â†’ Page 6/14... âœ“ 1822 chars\n",
      "      â†’ Page 7/14... âœ“ 1463 chars\n",
      "      â†’ Page 8/14... âœ“ 638 chars\n",
      "      â†’ Page 9/14... âœ“ 458 chars\n",
      "      â†’ Page 10/14... âœ“ 833 chars\n",
      "      â†’ Page 11/14... âœ“ 947 chars\n",
      "      â†’ Page 12/14... âœ“ 226 chars\n",
      "      â†’ Page 13/14... âœ“ 1496 chars\n",
      "      â†’ Page 14/14... âœ“ 2637 chars\n",
      "    â†’ OCR complete: 16262 characters\n",
      "  âœ“ OCR complete (14 pages, 16262 chars)\n",
      "      â†’ Extracting IDs...\n",
      "      âœ“ IDs extracted\n",
      "      â†’ Extracting land/valuation data...\n",
      "      âœ“ Land data extracted\n",
      "âœ“ Saved â†’ ./additional_2025v1\\doc42282020251030133255.json\n",
      "âš  Supabase upload failed: Supabase upload failed: {'message': \"Could not find the 'drainage' column of 'nrb_2025' in the schema cache\", 'code': 'P\n",
      "\n",
      "ðŸŽ‰ Batch finished in 1.5 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'filename': 'doc42281920251030133205.pdf',\n",
       "  'success': True,\n",
       "  'data': {'property_id': 'doc42281920251030133205',\n",
       "   'report_reference': 'SOO/DOO/5385/1/25',\n",
       "   'title_number': 'NAIROBI/BLOCK 75/1031',\n",
       "   'lr_number': '',\n",
       "   'ir_number': '',\n",
       "   'client_name': 'Corporate Recoveries, KCB Bank Kenya Limited',\n",
       "   'valuer_name': '',\n",
       "   'inspection_date': '2025/08/29',\n",
       "   'valuation_date': '',\n",
       "   'location_county': 'Nairobi County',\n",
       "   'location_description': 'Mumias South Road, to the immediate North of Shell Petrol Station, opposite Bidii Primary School and approximately 140 meters to the North of T-Square Mall within Buruburu Area',\n",
       "   'location_coordinates': '1Â°17\\'09.3\"S 36Â°52\\'49.6\"E',\n",
       "   'plot_area_hectares': 0.1551,\n",
       "   'plot_area_acres': 0.3832,\n",
       "   'land_use': 'mixed use development comprising a hotel facility and shops',\n",
       "   'plot_shape': 'rectangular-shaped',\n",
       "   'soil_type': 'ted soils',\n",
       "   'gradient': 'fairly gentle',\n",
       "   'drainage': None,\n",
       "   'vegetation': None,\n",
       "   'tenure_type': 'leasehold',\n",
       "   'registered_proprietor': 'James Kiniiya Gachiri',\n",
       "   'ownership_type': None,\n",
       "   'encumbrances': 'charge dated January 27, 20214 in favour of National Bank of Kenya Limited to secure a sum of Kshs. 230,000,00.00',\n",
       "   'market_value_amount': 0,\n",
       "   'market_value_currency': 'KShs',\n",
       "   'metadata': {'source_file': 'doc42281920251030133205.pdf',\n",
       "    'file_size_kb': 11013.9,\n",
       "    'processing_time_seconds': 54.0,\n",
       "    'ocr_used': True,\n",
       "    'pages_processed': 21,\n",
       "    'model_name': 'models/gemini-2.5-flash',\n",
       "    'timestamp': '2025-12-04T09:38:07.263939'}},\n",
       "  'timing': {'processing_seconds': 54.0}},\n",
       " {'filename': 'doc42282020251030133255.pdf',\n",
       "  'success': True,\n",
       "  'data': {'property_id': 'doc42282020251030133255',\n",
       "   'report_reference': 'SOO/DOO/5386/1/25',\n",
       "   'title_number': 'MATUNGULU/KAMBUSU/870',\n",
       "   'lr_number': '',\n",
       "   'ir_number': '',\n",
       "   'client_name': 'KCB Bank Kenya Limited',\n",
       "   'valuer_name': '',\n",
       "   'inspection_date': '2025/10/13',\n",
       "   'valuation_date': '',\n",
       "   'location_county': 'Machakos County',\n",
       "   'location_description': 'approximately 5 kilometres off Kangundo Road, 7 kilometres due Northeast of Tala shopping Center, 2 kilometres to the Northeast of Matungulu Boysâ€™ High School and 360 meters to Southeast of Kambusu Secondary School within Kambusu area',\n",
       "   'location_coordinates': '1Â°15\\'07.9\"S 37Â°21\\'54.9\"E',\n",
       "   'plot_area_hectares': 0.18,\n",
       "   'plot_area_acres': 0.44,\n",
       "   'land_use': 'Agricultural',\n",
       "   'plot_shape': 'irregular-shaped',\n",
       "   'soil_type': 'red soils',\n",
       "   'gradient': 'southward slope',\n",
       "   'drainage': None,\n",
       "   'vegetation': None,\n",
       "   'tenure_type': 'freehold interest',\n",
       "   'registered_proprietor': 'John Katunga Kavatha',\n",
       "   'ownership_type': None,\n",
       "   'encumbrances': 'Registered against the title is a charge dated 27/02/2024 in favour of KCB Bank Kenya Limited to secure a sum of KShs. 517,500.00.',\n",
       "   'market_value_amount': 700000.0,\n",
       "   'market_value_currency': 'KShs',\n",
       "   'metadata': {'source_file': 'doc42282020251030133255.pdf',\n",
       "    'file_size_kb': 7472.84,\n",
       "    'processing_time_seconds': 35.58,\n",
       "    'ocr_used': True,\n",
       "    'pages_processed': 14,\n",
       "    'model_name': 'models/gemini-2.5-flash',\n",
       "    'timestamp': '2025-12-04T09:38:43.414744'}},\n",
       "  'timing': {'processing_seconds': 35.58}}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

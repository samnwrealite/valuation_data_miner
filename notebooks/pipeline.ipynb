{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "873507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n",
      "  Gemini Model: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"test\": 123}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"property_id\", \"valuation_report\", \"property_details\", \"valuations\"],\n",
    "    \"properties\": {\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"valuation_report\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"report_reference\": {\"type\": \"string\"},\n",
    "                \"valuer\": {\"type\": \"string\"},\n",
    "                \"valuers\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"string\"},\n",
    "                            \"qualification\": {\"type\": \"string\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inspection_date\": {\"type\": \"string\"},\n",
    "                \"report_date\": {\"type\": \"string\"},\n",
    "                \"client\": {\"type\": \"string\"},\n",
    "                \"client_address\": {\"type\": \"string\"},\n",
    "                \"purpose\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_details\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"apartment_number\": {\"type\": \"string\"},\n",
    "                \"block\": {\"type\": \"string\"},\n",
    "                \"floor\": {\"type\": \"string\"},\n",
    "                \"title_details\": {\"type\": \"object\"},\n",
    "                \"location\": {\"type\": \"object\"},\n",
    "                \"tenure\": {\"type\": \"object\"},\n",
    "                \"registered_proprietors\": {\"type\": \"array\"},\n",
    "                \"ownership_type\": {\"type\": \"string\"},\n",
    "                \"encumbrances\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_description\": {\"type\": \"object\"},\n",
    "        \"apartment_details\": {\"type\": \"object\"},\n",
    "        \"occupancy\": {\"type\": \"string\"},\n",
    "        \"condition\": {\"type\": \"string\"},\n",
    "        \"market_assessment\": {\"type\": \"object\"},\n",
    "        \"valuations\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"current_market_value\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"amount\": {\"type\": \"number\"},\n",
    "                        \"currency\": {\"type\": \"string\"},\n",
    "                        \"amount_words\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"valuation_methodology\": {\"type\": \"array\"},\n",
    "        \"lease_details\": {\"type\": \"object\"},\n",
    "        \"compliance\": {\"type\": \"object\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ JSON schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase client initialized\n"
     ]
    }
   ],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase functions defined\n"
     ]
    }
   ],
   "source": [
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload to Supabase\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = data.copy()\n",
    "        upload_data['source_filename'] = filename\n",
    "        upload_data['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        client.table(\"property_valuations\").insert(upload_data).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Supabase functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=300, fmt='jpeg', thread_count=2)\n",
    "        \n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        # OCR each page\n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 1'\n",
    "            )\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ OCR function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\"  # safer, available model\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise Exception(\"GEMINI_API_KEY not found in environment\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def chunk_text(text: str, max_chars: int = 12000) -> List[str]:\n",
    "    \"\"\"Split long text into smaller chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chars\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response.\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    if not response_text.startswith('{'):\n",
    "        start = response_text.find('{')\n",
    "        end = response_text.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Recursively merge two dictionaries.\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if (\n",
    "            k in d1 and isinstance(d1[k], dict) and isinstance(v, dict)\n",
    "        ):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# EXTRACTION\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"Extract structured data from a document with chunking and retries.\"\"\"\n",
    "    chunks = chunk_text(text_content, max_chars=12000)\n",
    "    combined_result = {}\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = f\"\"\"Extract property valuation data from this Kenyan document (OCR text). Return ONLY valid JSON.\n",
    "\n",
    "RULES:\n",
    "- Extract EXACT values from document (no placeholders)\n",
    "- If field not found, use \"\" or {{}}\n",
    "- Amounts as numbers without commas\n",
    "- Dates as YYYY-MM-DD\n",
    "\n",
    "Schema:\n",
    "{{\n",
    "  \"property_id\": \"string\",\n",
    "  \"valuation_report\": {{\"report_reference\": \"\", \"valuer\": \"\", \"valuers\": [], \"inspection_date\": \"\",\n",
    "                        \"report_date\": \"\", \"client\": \"\", \"client_address\": \"\", \"purpose\": \"\"}},\n",
    "  \"property_details\": {{\"apartment_number\": \"\", \"block\": \"\", \"floor\": \"\",\n",
    "                        \"title_details\": {{}}, \"location\": {{}}, \"tenure\": {{}} ,\n",
    "                        \"registered_proprietors\": [], \"ownership_type\": \"\", \"encumbrances\": \"\"}},\n",
    "  \"property_description\": {{}} ,\n",
    "  \"apartment_details\": {{}} ,\n",
    "  \"occupancy\": \"\",\n",
    "  \"condition\": \"\",\n",
    "  \"market_assessment\": {{}} ,\n",
    "  \"valuations\": {{\"current_market_value\": {{\"amount\": 0, \"currency\": \"KES\", \"amount_words\": \"\"}}}} ,\n",
    "  \"valuation_methodology\": [] ,\n",
    "  \"lease_details\": {{}} ,\n",
    "  \"compliance\": {{}}\n",
    "}}\n",
    "\n",
    "Document:\n",
    "{chunk}\n",
    "\n",
    "Return ONLY JSON:\"\"\"\n",
    "\n",
    "        response_text = \"\"\n",
    "        success = False\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "                start_time = time.time()\n",
    "                response = model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=genai.types.GenerationConfig(\n",
    "                        temperature=0.1,\n",
    "                        max_output_tokens=8192,\n",
    "                    )\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                if hasattr(response, \"text\") and response.text:\n",
    "                    response_text = clean_gemini_json(response.text)\n",
    "                    extracted_data = json.loads(response_text)\n",
    "                    combined_result = deep_merge(combined_result, extracted_data)\n",
    "                    print(f\"Chunk {i+1}/{len(chunks)} succeeded in {elapsed:.1f}s, attempt {attempt+1}\")\n",
    "                    success = True\n",
    "                    break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Chunk {i} JSON decode failed, attempt {attempt+1}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Chunk {i} Gemini error, attempt {attempt+1}: {e}\")\n",
    "\n",
    "        if not success:\n",
    "            print(f\"âš  Chunk {i} failed after 3 attempts\")\n",
    "\n",
    "        # Save response for debugging\n",
    "        debug_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_gemini_response_chunk{i+1}.txt\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response_text or \"\")\n",
    "\n",
    "    # Ensure property_id exists\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "\n",
    "    return combined_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in the PDF_DIR.\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    for idx, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_file.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # OCR text extraction (you already have your OCR function)\n",
    "        text_content = process_scanned_pdf_with_ocr(str(pdf_file)) \n",
    "\n",
    "        # Gemini extraction\n",
    "        data = extract_with_gemini(text_content, pdf_file.name)\n",
    "\n",
    "        # Save JSON\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(pdf_file).stem}.json\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        result = {\n",
    "            \"filename\": pdf_file.name,\n",
    "            \"success\": bool(data),\n",
    "            \"data\": data,\n",
    "            \"timing\": {\"total\": 0},  # optionally add timing\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"âœ“ Saved to {output_file}\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nBatch completed in {total_time/60:.1f} min\")\n",
    "\n",
    "    # Save batch summary\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª TESTING: doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf\n",
      "\n",
      "\n",
      "  ðŸ“„ Stage 1: OCR\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 12 pages...\n",
      "      â†’ Page 1/12... âœ“ 1266 chars\n",
      "      â†’ Page 2/12... âœ“ 2401 chars\n",
      "      â†’ Page 3/12... âœ“ 165 chars\n",
      "      â†’ Page 4/12... âœ“ 182 chars\n",
      "      â†’ Page 5/12... âœ“ 1357 chars\n",
      "      â†’ Page 6/12... âœ“ 1800 chars\n",
      "      â†’ Page 7/12... âœ“ 1664 chars\n",
      "      â†’ Page 8/12... âœ“ 539 chars\n",
      "      â†’ Page 9/12... âœ“ 491 chars\n",
      "      â†’ Page 10/12... âœ“ 581 chars\n",
      "      â†’ Page 11/12... âœ“ 1039 chars\n",
      "      â†’ Page 12/12... âœ“ 607 chars\n",
      "    â†’ OCR complete: 12297 characters\n",
      "  âœ“ 12297 chars in 52.2s\n",
      "\n",
      "  ðŸ¤– Stage 2: Gemini\n",
      "Chunk 1/1 responded in 29.7s, attempt 1\n",
      "  âœ“ Extracted in 29.7s\n",
      "\n",
      "  âœ… Stage 3: Validation\n",
      "  âš  Warning: 'Leasehold' is not of type 'object'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['property_deta\n",
      "\n",
      "  ðŸ’¾ Stage 4: Save\n",
      "  âœ“ Saved to doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.json\n",
      "\n",
      "âœ… TEST PASSED!\n",
      "\n",
      "Preview:\n",
      "{\n",
      "  \"property_id\": \"L.R. NUMBER: 7815/83 (I.R NUMBER: 237281)\",\n",
      "  \"valuation_report\": {\n",
      "    \"report_reference\": \"SOO/DOO/5296/1/25\",\n",
      "    \"valuer\": \"Simon Oruka Orwa\",\n",
      "    \"valuers\": [\n",
      "      \"Ken Wathome\",\n",
      "      \"Nahashon Kuria\",\n",
      "      \"Simon Oruka Orwa\"\n",
      "    ],\n",
      "    \"inspection_date\": \"14th April 2021\",\n",
      "    \"report_date\": \"18 April 2021\",\n",
      "    \"client\": \"Mr. Kenneth Wathome\",\n",
      "    \"client_address\": \"Nairobi\",\n",
      "    \"purpose\": \"advisory purposes\"\n",
      "  },\n",
      "  \"property_details\": {\n",
      "    \"apartment_number\": \"\",\n",
      "    \"block\": \"\",\n",
      "    \"floor\": \"\",\n",
      "    \"title_details\": {\n",
      "      \"title_number\": \"IR.237281\",\n",
      "      \"land_reference_number\": \"7815/83\",\n",
      "      \"term\": \"999 years\",\n",
      "      \"annual_ground_rent\": \"30,258.00\",\n",
      "      \"deed_plan_number\": \"F/R No. 507/87\",\n",
      "      \"survey_plan_number\": \"F/R No. 507/87, 453369\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"county\": \"Machakos County\",\n",
      "      \"area\": \"Lukenya Area\",\n",
      "      \"coordinates\": \"1\\u00b028'12.2\\\"S 37\\u00b002'21.7\\\"E\",\n",
      "      \"description\": \"The property is situated ap\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': 'doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf',\n",
       " 'success': True,\n",
       " 'stage': 'saving',\n",
       " 'error': None,\n",
       " 'data': {'property_id': 'L.R. NUMBER: 7815/83 (I.R NUMBER: 237281)',\n",
       "  'valuation_report': {'report_reference': 'SOO/DOO/5296/1/25',\n",
       "   'valuer': 'Simon Oruka Orwa',\n",
       "   'valuers': ['Ken Wathome', 'Nahashon Kuria', 'Simon Oruka Orwa'],\n",
       "   'inspection_date': '14th April 2021',\n",
       "   'report_date': '18 April 2021',\n",
       "   'client': 'Mr. Kenneth Wathome',\n",
       "   'client_address': 'Nairobi',\n",
       "   'purpose': 'advisory purposes'},\n",
       "  'property_details': {'apartment_number': '',\n",
       "   'block': '',\n",
       "   'floor': '',\n",
       "   'title_details': {'title_number': 'IR.237281',\n",
       "    'land_reference_number': '7815/83',\n",
       "    'term': '999 years',\n",
       "    'annual_ground_rent': '30,258.00',\n",
       "    'deed_plan_number': 'F/R No. 507/87',\n",
       "    'survey_plan_number': 'F/R No. 507/87, 453369'},\n",
       "   'location': {'county': 'Machakos County',\n",
       "    'area': 'Lukenya Area',\n",
       "    'coordinates': '1Â°28\\'12.2\"S 37Â°02\\'21.7\"E',\n",
       "    'description': 'The property is situated approximately 700 meters off Mombasa Road, 2.3 kilometres to the East of Greenpark Estate and 2.5 kilometres due South of Lukenya Gateway within Lukenya Area of Machakos County.'},\n",
       "   'tenure': 'Leasehold',\n",
       "   'registered_proprietors': ['Kenneth Wathome Mwatu'],\n",
       "   'ownership_type': 'Private',\n",
       "   'encumbrances': 'Unascertained. Caveat registered as IR.8069/40'},\n",
       "  'property_description': {'plot_area': '2.024 hectares (5.001 acres)',\n",
       "   'land_use': 'Mixed use',\n",
       "   'parcel_shape': 'Rectangular',\n",
       "   'soil_type': 'Black cotton soils',\n",
       "   'slope': 'Fairly level',\n",
       "   'boundaries': 'Marked by corner beacons',\n",
       "   'services': 'Mains electricity and water are available for connection. Foul drainage would be into a septic tank.',\n",
       "   'access_road': 'Earth surfaced branching off the tar-surfaced Mombasa Road.',\n",
       "   'improvements': 'No structural improvements',\n",
       "   'general_remarks': 'This is a vacant parcel of land situated off Mombasa Road, within Lukenya Area of Machakos County. The property is accessible and in proximity to major road networks and other social activities to support any developments on the plot. The neighbourhood within which the subject property is located was hitherto agricultural. However, this is changing giving way to a myriad of mixed-use developments comprising residential, commercial and industrial users. This has pushed property prices upwards.'},\n",
       "  'apartment_details': {},\n",
       "  'occupancy': 'Vacant',\n",
       "  'condition': 'Undeveloped land',\n",
       "  'market_assessment': {'neighbourhood_description': 'The neighbourhood within which the subject property is located was hitherto agricultural. However, this is changing giving way to a myriad of mixed-use developments comprising residential, commercial and industrial users.',\n",
       "   'property_price_trend': 'Pushed property prices upwards.',\n",
       "   'demand': 'Stable',\n",
       "   'market_outlook': 'Lukewarm in the short and medium term due to the depressed economy that has been precipitated by the Covid 19 pandemic.'},\n",
       "  'valuations': {'current_market_value': {'amount': 60000000,\n",
       "    'currency': 'KES',\n",
       "    'amount_words': 'Kenya Shillings Sixty Million'}},\n",
       "  'valuation_methodology': ['Comparative Sales Method'],\n",
       "  'lease_details': {'lease_term': '999 years',\n",
       "   'lease_start_date': '1948-08-01',\n",
       "   'annual_rent': '30,258.00'},\n",
       "  'compliance': {'public_utility_user': 'The property has not been set aside for public use/water catchment area, is not government land, and has not been reserved for public utility user. It is not listed in the Report of the Commission of Inquiry into the Illegal/Irregular Allocation of Public Land (Ndungu Report) Annexes Volumes 1, 2 & 3, nor adversely mentioned in NLC Kenya Gazette Notices No. 6862 of 17th July 2017, 12526 of 22nd December 2017, 2032 and 2033 of 2nd March 2018, and 1547 of 15th February 2019. The land is not on riparian reserve / wetland.',\n",
       "   'planning_highway_statutory': 'No significant prospect of or potential for change of use is foreseen. The current user of the property conforms to its stipulated use.',\n",
       "   'environmental_issues': 'No obvious visible environmental issues including contaminated land either on the subject or on an adjoining site that materially affect the value of the properties. A detailed environmental impact assessment report would verify this in greater details.'}},\n",
       " 'timing': {'ocr': 52.170814990997314,\n",
       "  'gemini': 29.67353367805481,\n",
       "  'total': 81.8781042098999}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ PROCESSING 47 PDFs\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "[1/47] doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 12 pages...\n",
      "      â†’ Page 1/12... âœ“ 1266 chars\n",
      "      â†’ Page 2/12... âœ“ 2401 chars\n",
      "      â†’ Page 3/12... âœ“ 165 chars\n",
      "      â†’ Page 4/12... âœ“ 182 chars\n",
      "      â†’ Page 5/12... âœ“ 1357 chars\n",
      "      â†’ Page 6/12... âœ“ 1800 chars\n",
      "      â†’ Page 7/12... âœ“ 1664 chars\n",
      "      â†’ Page 8/12... âœ“ 539 chars\n",
      "      â†’ Page 9/12... âœ“ 491 chars\n",
      "      â†’ Page 10/12... âœ“ 581 chars\n",
      "      â†’ Page 11/12... âœ“ 1039 chars\n",
      "      â†’ Page 12/12... âœ“ 607 chars\n",
      "    â†’ OCR complete: 12297 characters\n",
      "Chunk 1/2 succeeded in 21.6s, attempt 1\n",
      "Chunk 2/2 succeeded in 5.5s, attempt 1\n",
      "âœ“ Saved to ./extracted_data\\doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.json\n",
      "\n",
      "======================================================================\n",
      "[2/47] doc39404020250117161224 (1) TITLE NO KARAI-GIKAMBURA-6448 ONDIRI AREA KIAMBU COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 21 pages...\n",
      "      â†’ Page 1/21... âœ“ 1370 chars\n",
      "      â†’ Page 2/21... âœ“ 2369 chars\n",
      "      â†’ Page 3/21... âœ“ 158 chars\n",
      "      â†’ Page 4/21... âœ“ 129 chars\n",
      "      â†’ Page 5/21... âœ“ 259 chars\n",
      "      â†’ Page 6/21... âœ“ 1294 chars\n",
      "      â†’ Page 7/21... âœ“ 1411 chars\n",
      "      â†’ Page 8/21... âœ“ 1053 chars\n",
      "      â†’ Page 9/21... âœ“ 840 chars\n",
      "      â†’ Page 10/21... âœ“ 1770 chars\n",
      "      â†’ Page 11/21... âœ“ 1817 chars\n",
      "      â†’ Page 12/21... âœ“ 890 chars\n",
      "      â†’ Page 13/21... âœ“ 623 chars\n",
      "      â†’ Page 14/21... âœ“ 1014 chars\n",
      "      â†’ Page 15/21... âœ“ 129 chars\n",
      "      â†’ Page 16/21... âœ“ 1219 chars\n",
      "      â†’ Page 17/21... âœ“ 669 chars\n",
      "      â†’ Page 18/21... âœ“ 744 chars\n",
      "      â†’ Page 19/21... âœ“ 280 chars\n",
      "      â†’ Page 20/21... âœ“ 172 chars\n",
      "      â†’ Page 21/21... âœ“ 1957 chars\n",
      "    â†’ OCR complete: 20534 characters\n",
      "Chunk 1/2 succeeded in 33.2s, attempt 1\n",
      "Chunk 2/2 succeeded in 27.9s, attempt 1\n",
      "âœ“ Saved to ./extracted_data\\doc39404020250117161224 (1) TITLE NO KARAI-GIKAMBURA-6448 ONDIRI AREA KIAMBU COUNTY.json\n",
      "\n",
      "======================================================================\n",
      "[3/47] doc39490220250127093903 TITLE NO KAJIADO DELALEKUTUK-14709 DELALEKTUK AREA KAJIADO COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 17 pages...\n",
      "      â†’ Page 1/17... âœ“ 1281 chars\n",
      "      â†’ Page 2/17... âœ“ 2360 chars\n",
      "      â†’ Page 3/17... âœ“ 101 chars\n",
      "      â†’ Page 4/17... âœ“ 90 chars\n",
      "      â†’ Page 5/17... âœ“ 1186 chars\n",
      "      â†’ Page 6/17... âœ“ 1352 chars\n",
      "      â†’ Page 7/17... âœ“ 1635 chars\n",
      "      â†’ Page 8/17... âœ“ 483 chars\n",
      "      â†’ Page 9/17... âœ“ 673 chars\n",
      "      â†’ Page 10/17... âœ“ 216 chars\n",
      "      â†’ Page 11/17... (empty)\n",
      "      â†’ Page 12/17... âœ“ 981 chars\n",
      "      â†’ Page 13/17... âœ“ 683 chars\n",
      "      â†’ Page 14/17... âœ“ 538 chars\n",
      "      â†’ Page 15/17... âœ“ 84 chars\n",
      "      â†’ Page 16/17... âœ“ 186 chars\n",
      "      â†’ Page 17/17... âœ“ 1707 chars\n",
      "    â†’ OCR complete: 13833 characters\n",
      "Chunk 1/2 succeeded in 26.6s, attempt 1\n",
      "Chunk 2/2 succeeded in 10.6s, attempt 1\n",
      "âœ“ Saved to ./extracted_data\\doc39490220250127093903 TITLE NO KAJIADO DELALEKUTUK-14709 DELALEKTUK AREA KAJIADO COUNTY.json\n",
      "\n",
      "======================================================================\n",
      "[4/47] doc39542520250203100758 NJORO-NGATA BLOCK 2-5138 RVIST AREA KIAMBU COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 16 pages...\n",
      "      â†’ Page 1/16... âœ“ 1465 chars\n",
      "      â†’ Page 2/16... âœ“ 2402 chars\n",
      "      â†’ Page 3/16... âœ“ 166 chars\n",
      "      â†’ Page 4/16... âœ“ 110 chars\n",
      "      â†’ Page 5/16... âœ“ 468 chars\n",
      "      â†’ Page 6/16... âœ“ 1432 chars\n",
      "      â†’ Page 7/16... âœ“ 1128 chars\n",
      "      â†’ Page 8/16... âœ“ 773 chars\n",
      "      â†’ Page 9/16... âœ“ 1228 chars\n",
      "      â†’ Page 10/16... âœ“ 1640 chars\n",
      "      â†’ Page 11/16... âœ“ 780 chars\n",
      "      â†’ Page 12/16... âœ“ 773 chars\n",
      "      â†’ Page 13/16... âœ“ 90 chars\n",
      "      â†’ Page 14/16... âœ“ 1239 chars\n",
      "      â†’ Page 15/16... âœ“ 945 chars\n",
      "      â†’ Page 16/16... âœ“ 783 chars\n",
      "    â†’ OCR complete: 15699 characters\n"
     ]
    }
   ],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

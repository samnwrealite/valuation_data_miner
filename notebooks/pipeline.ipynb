{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/site-packages (11.3.0)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/site-packages (from google-generativeai) (4.15.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in /usr/local/lib/python3.11/site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, proto-plus, googleapis-common-protos, cachetools, rsa, pyasn1-modules, httplib2, grpcio-status, google-auth, pdfplumber, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/17\u001b[0m [google-generativeai]ogle-ai-generativelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-6.2.2 google-ai-generativelanguage-0.6.15 google-api-core-2.28.1 google-api-python-client-2.187.0 google-auth-2.43.0 google-auth-httplib2-0.2.1 google-generativeai-0.8.5 googleapis-common-protos-1.72.0 grpcio-status-1.71.2 httplib2-0.31.0 pdfplumber-0.11.8 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.5 rsa-4.9.1 uritemplate-4.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n",
      "  Gemini Model: gemini-1.5-flash\n",
      "\n",
      "âš  WARNING: GEMINI_API_KEY not found!\n",
      "  Get key from: https://makersuite.google.com/app/apikey\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"  # Fast and free!\n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"property_id\", \"valuation_report\", \"property_details\", \"valuations\"],\n",
    "    \"properties\": {\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"valuation_report\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"report_reference\": {\"type\": \"string\"},\n",
    "                \"valuer\": {\"type\": \"string\"},\n",
    "                \"valuers\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"string\"},\n",
    "                            \"qualification\": {\"type\": \"string\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inspection_date\": {\"type\": \"string\"},\n",
    "                \"report_date\": {\"type\": \"string\"},\n",
    "                \"client\": {\"type\": \"string\"},\n",
    "                \"client_address\": {\"type\": \"string\"},\n",
    "                \"purpose\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_details\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"apartment_number\": {\"type\": \"string\"},\n",
    "                \"block\": {\"type\": \"string\"},\n",
    "                \"floor\": {\"type\": \"string\"},\n",
    "                \"title_details\": {\"type\": \"object\"},\n",
    "                \"location\": {\"type\": \"object\"},\n",
    "                \"tenure\": {\"type\": \"object\"},\n",
    "                \"registered_proprietors\": {\"type\": \"array\"},\n",
    "                \"ownership_type\": {\"type\": \"string\"},\n",
    "                \"encumbrances\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_description\": {\"type\": \"object\"},\n",
    "        \"apartment_details\": {\"type\": \"object\"},\n",
    "        \"occupancy\": {\"type\": \"string\"},\n",
    "        \"condition\": {\"type\": \"string\"},\n",
    "        \"market_assessment\": {\"type\": \"object\"},\n",
    "        \"valuations\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"current_market_value\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"amount\": {\"type\": \"number\"},\n",
    "                        \"currency\": {\"type\": \"string\"},\n",
    "                        \"amount_words\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"valuation_methodology\": {\"type\": \"array\"},\n",
    "        \"lease_details\": {\"type\": \"object\"},\n",
    "        \"compliance\": {\"type\": \"object\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ JSON schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Supabase credentials not configured\n"
     ]
    }
   ],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase functions defined\n"
     ]
    }
   ],
   "source": [
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload to Supabase\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = data.copy()\n",
    "        upload_data['source_filename'] = filename\n",
    "        upload_data['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        client.table(\"property_valuations\").insert(upload_data).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Supabase functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=300, fmt='jpeg', thread_count=2)\n",
    "        \n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        # OCR each page\n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 1'\n",
    "            )\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ OCR function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72030385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gemini extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"Extract structured data using Google Gemini\"\"\"\n",
    "    import google.generativeai as genai\n",
    "    \n",
    "    if not GEMINI_API_KEY:\n",
    "        raise Exception(\"GEMINI_API_KEY not found\")\n",
    "    \n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    text_sample = text_content[:80000] if len(text_content) > 80000 else text_content\n",
    "    \n",
    "    print(f\"    â†’ Sending {len(text_sample)} chars to Gemini...\")\n",
    "    \n",
    "    prompt = f\"\"\"Extract property valuation data from this Kenyan document (OCR text). Return ONLY valid JSON.\n",
    "\n",
    "RULES:\n",
    "- Extract EXACT values from document (no placeholders like \"John Doe\")\n",
    "- If field not found, use \"\" or {{}}\n",
    "- Amounts as numbers without commas (8500000 not \"8,500,000\")\n",
    "- Dates as YYYY-MM-DD\n",
    "\n",
    "Schema:\n",
    "{{\n",
    "  \"property_id\": \"string\",\n",
    "  \"valuation_report\": {{\n",
    "    \"report_reference\": \"\", \"valuer\": \"\", \"valuers\": [], \"inspection_date\": \"\",\n",
    "    \"report_date\": \"\", \"client\": \"\", \"client_address\": \"\", \"purpose\": \"\"\n",
    "  }},\n",
    "  \"property_details\": {{\n",
    "    \"apartment_number\": \"\", \"block\": \"\", \"floor\": \"\",\n",
    "    \"title_details\": {{}}, \"location\": {{}}, \"tenure\": {{}},\n",
    "    \"registered_proprietors\": [], \"ownership_type\": \"\", \"encumbrances\": \"\"\n",
    "  }},\n",
    "  \"property_description\": {{}},\n",
    "  \"apartment_details\": {{}},\n",
    "  \"occupancy\": \"\",\n",
    "  \"condition\": \"\",\n",
    "  \"market_assessment\": {{}},\n",
    "  \"valuations\": {{\n",
    "    \"current_market_value\": {{\"amount\": 0, \"currency\": \"KES\", \"amount_words\": \"\"}}\n",
    "  }},\n",
    "  \"valuation_methodology\": [],\n",
    "  \"lease_details\": {{}},\n",
    "  \"compliance\": {{}}\n",
    "}}\n",
    "\n",
    "Document:\n",
    "{text_sample}\n",
    "\n",
    "Return ONLY JSON:\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=4096,\n",
    "            )\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        print(f\"    â†’ Gemini responded in {elapsed:.1f}s\")\n",
    "        \n",
    "        # Save response\n",
    "        debug_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_gemini_response.txt\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response_text)\n",
    "        \n",
    "        # Clean JSON\n",
    "        if '```json' in response_text:\n",
    "            response_text = response_text.split('```json')[1].split('```')[0]\n",
    "        elif '```' in response_text:\n",
    "            response_text = response_text.split('```')[1].split('```')[0]\n",
    "        \n",
    "        response_text = response_text.strip()\n",
    "        \n",
    "        if not response_text.startswith('{'):\n",
    "            start = response_text.find('{')\n",
    "            end = response_text.rfind('}')\n",
    "            if start != -1 and end != -1:\n",
    "                response_text = response_text[start:end+1]\n",
    "        \n",
    "        extracted_data = json.loads(response_text)\n",
    "        \n",
    "        if not extracted_data.get('property_id'):\n",
    "            extracted_data['property_id'] = Path(filename).stem\n",
    "        \n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        raise Exception(f\"JSON parsing failed: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Gemini error: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Gemini extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Batch processing defined\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs in {PDF_DIR}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    for idx, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_file.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        result = process_single_pdf_complete(str(pdf_file), pdf_file.name)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"\\nâœ… SUCCESS - {result['timing']['total']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ FAILED\")\n",
    "    \n",
    "    # Summary\n",
    "    total_time = time.time() - total_start\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“Š SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total: {len(results)} | Success: {successful} | Failed: {len(results)-successful}\")\n",
    "    print(f\"Time: {total_time/60:.1f} min | Avg: {total_time/len(results):.1f}s/PDF\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': GEMINI_MODEL,\n",
    "            'total': len(results),\n",
    "            'successful': successful,\n",
    "            'results': results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Batch processing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… ALL FUNCTIONS LOADED\n",
      "======================================================================\n",
      "No PDFs found\n"
     ]
    }
   ],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs in C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa54f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

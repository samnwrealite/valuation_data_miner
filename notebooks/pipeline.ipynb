{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: Z:\\Risper M\\2025 valuation reports\\Year 2025\n",
      "  Gemini Model: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"Z:\\Risper M\\2025 valuation reports\\Year 2025\"\n",
    "OUTPUT_DIR = \"./additional_2025v1\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "# print(f\"Key: {SUPABASE_KEY}, \\n URL: {SUPABASE_URL}\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m genai.configure(api_key=GEMINI_API_KEY)\n\u001b[32m      4\u001b[39m models = genai.list_models()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\generativeai\\models.py:204\u001b[39m, in \u001b[36mlist_models\u001b[39m\u001b[34m(page_size, client, request_options)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    202\u001b[39m     client = get_default_model_client()\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    205\u001b[39m     model = \u001b[38;5;28mtype\u001b[39m(model).to_dict(model)\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_types.Model(**model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\client.py:923\u001b[39m, in \u001b[36mModelServiceClient.list_models\u001b[39m\u001b[34m(self, request, page_size, page_token, retry, timeout, metadata)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# This method is paged; wrap the response in a pager, which provides\u001b[39;00m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# an `__iter__` convenience method.\u001b[39;00m\n\u001b[32m    932\u001b[39m response = pagers.ListModelsPager(\n\u001b[32m    933\u001b[39m     method=rpc,\n\u001b[32m    934\u001b[39m     request=request,\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m     metadata=metadata,\n\u001b[32m    939\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"property_id\",\n",
    "        \"report_reference\",\n",
    "        \"market_value_amount\",\n",
    "        \"metadata\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "\n",
    "        # --- CORE IDENTIFIERS ---\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"report_reference\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- TITLE INFORMATION ---\n",
    "        \"title_number\": {\"type\": \"string\"},\n",
    "        \"lr_number\": {\"type\": \"string\"},\n",
    "        \"ir_number\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- CLIENT + VALUER ---\n",
    "        \"client_name\": {\"type\": \"string\"},\n",
    "        \"valuer_name\": {\"type\": \"string\"},\n",
    "        \"inspection_date\": {\"type\": \"string\"},\n",
    "        \"valuation_date\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- LOCATION ---\n",
    "        \"location_county\": {\"type\": \"string\"},\n",
    "        \"location_description\": {\"type\": \"string\"},\n",
    "        \"location_coordinates\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- PARCEL DETAILS ---\n",
    "        \"plot_area_hectares\": {\"type\": \"number\"},\n",
    "        \"plot_area_acres\": {\"type\": \"number\"},\n",
    "        \"land_use\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- NEW LAND PHYSICAL FEATURES ---\n",
    "        \"plot_shape\": {\"type\": \"string\"},     # rectangular, irregular, square\n",
    "        \"soil_type\": {\"type\": \"string\"},      # black cotton, red soil, sandy, loam, murram\n",
    "        \"gradient\": {\"type\": \"string\"},       # flat, gentle slope, moderate slope, steep\n",
    "        \"drainage\": {\"type\": \"string\"},       # optional recommendation\n",
    "        \"vegetation\": {\"type\": \"string\"},     # optional recommendation\n",
    "\n",
    "        # --- TENURE + OWNERSHIP ---\n",
    "        \"tenure_type\": {\"type\": \"string\"},\n",
    "        \"registered_proprietor\": {\"type\": \"string\"},\n",
    "        \"ownership_type\": {\"type\": \"string\"},\n",
    "        \"encumbrances\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- VALUATION ---\n",
    "        \"market_value_amount\": {\"type\": \"number\"},\n",
    "        \"market_value_currency\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- METADATA ---\n",
    "        \"metadata\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"source_file\": {\"type\": \"string\"},\n",
    "                \"file_size_kb\": {\"type\": \"number\"},\n",
    "                \"processing_time_seconds\": {\"type\": \"number\"},\n",
    "                \"ocr_used\": {\"type\": \"boolean\"},\n",
    "                \"pages_processed\": {\"type\": \"number\"},\n",
    "                \"model_name\": {\"type\": \"string\"},\n",
    "                \"timestamp\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Updated JSON schema defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_TABLE = \"nrb_2025\"   \n",
    "\n",
    "\n",
    "def sanitize_dates_for_supabase(data: dict) -> dict:\n",
    "    \"\"\"Convert empty string dates to None for Supabase insert\"\"\"\n",
    "    date_fields = [\"inspection_date\", \"valuation_date\"]\n",
    "    for field in date_fields:\n",
    "        if field in data and (data[field] == \"\" or data[field] is None):\n",
    "            data[field] = None\n",
    "    return data\n",
    "\n",
    "\n",
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload extracted valuation data to Supabase (restricted to valid columns).\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = sanitize_dates_for_supabase(data.copy())\n",
    "\n",
    "        # Allowed columns EXACTLY as in your Supabase table\n",
    "        allowed_columns = {\n",
    "            \"property_id\",\n",
    "            \"report_reference\",\n",
    "            \"title_number\",\n",
    "            \"lr_number\",\n",
    "            \"ir_number\",\n",
    "            \"client_name\",\n",
    "            \"valuer_name\",\n",
    "            \"inspection_date\",\n",
    "            \"valuation_date\",\n",
    "            \"location_county\",\n",
    "            \"location_description\",\n",
    "            \"location_coordinates\",\n",
    "            \"plot_area_hectares\",\n",
    "            \"plot_area_acres\",\n",
    "            \"land_use\",\n",
    "            \"plot_shape\",\n",
    "            \"soil_type\",\n",
    "            \"gradient\",\n",
    "            \"drainage\",\n",
    "            \"vegetation\",\n",
    "            \"tenure_type\",\n",
    "            \"registered_proprietor\",\n",
    "            \"ownership_type\",\n",
    "            \"encumbrances\",\n",
    "            \"market_value_amount\",\n",
    "            \"market_value_currency\",\n",
    "            \"metadata\"\n",
    "        }\n",
    "\n",
    "        # Remove any field not in table\n",
    "        clean_record = {k: v for k, v in upload_data.items() if k in allowed_columns}\n",
    "\n",
    "        # Insert into your table\n",
    "        client.table(SUPABASE_TABLE).insert(clean_record).execute()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str, dpi: int = 150) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR safely (memory-friendly).\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        import gc\n",
    "\n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images at {dpi} dpi...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, fmt='jpeg', thread_count=2)\n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            # OCR the page (use psm 6 for memory efficiency)\n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 6'\n",
    "            ).strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "            \n",
    "            # Free memory after each page\n",
    "            image.close()\n",
    "            del image\n",
    "            gc.collect()\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output for debugging\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Updated OCR function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd186301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "from time import sleep\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def clean_text_for_model(text: str) -> str:\n",
    "    \"\"\"Remove page headers, footers, and empty lines\"\"\"\n",
    "    text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_report_reference(text: str) -> str:\n",
    "    \"\"\"Detect report reference numbers\"\"\"\n",
    "    pattern = r\"\\b(?:[A-Z]{2,3}/)?(?:[A-Z]{2,3}/)?\\d{3,5}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        return max(matches, key=len)\n",
    "    return \"\"\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}')\n",
    "    if start != -1 and end != -1:\n",
    "        response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Merge dictionaries recursively\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# CRITICAL: Configure safety settings\n",
    "# -----------------------------\n",
    "def get_safe_generation_config():\n",
    "    \"\"\"Returns generation config with safety settings disabled\"\"\"\n",
    "    safety_settings = [\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "            \"threshold\": \"BLOCK_NONE\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=4096,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    return generation_config, safety_settings\n",
    "\n",
    "# -----------------------------\n",
    "# Rate limiting with retry logic\n",
    "# -----------------------------\n",
    "def call_gemini_with_retry(model, prompt, generation_config, safety_settings, max_retries=3):\n",
    "    \"\"\"\n",
    "    Call Gemini API with automatic retry on rate limit errors.\n",
    "    Implements exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            # Check if response was blocked\n",
    "            if response.prompt_feedback.block_reason:\n",
    "                print(f\"      âš  Response blocked: {response.prompt_feedback.block_reason}\")\n",
    "                return None\n",
    "            \n",
    "            # Check if we have valid text\n",
    "            if hasattr(response, 'text'):\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"      âš  No text in response\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            \n",
    "            # Check if it's a rate limit error (429)\n",
    "            if \"429\" in error_str or \"quota\" in error_str.lower():\n",
    "                # Extract retry delay from error message\n",
    "                import re\n",
    "                retry_match = re.search(r'retry in (\\d+\\.\\d+)s', error_str)\n",
    "                \n",
    "                if retry_match:\n",
    "                    retry_seconds = float(retry_match.group(1))\n",
    "                else:\n",
    "                    # Default exponential backoff: 5s, 15s, 30s\n",
    "                    retry_seconds = 5 * (3 ** attempt)\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"      âš  Rate limit hit. Waiting {retry_seconds:.1f}s before retry {attempt+1}/{max_retries}...\")\n",
    "                    sleep(retry_seconds)\n",
    "                else:\n",
    "                    print(f\"      âœ— Rate limit exceeded after {max_retries} attempts\")\n",
    "                    raise e\n",
    "            else:\n",
    "                # Non-rate-limit error, raise immediately\n",
    "                raise e\n",
    "    \n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction function with rate limiting\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract structured property valuation data from a Kenyan PDF OCR text.\n",
    "    Two-pass approach with automatic rate limiting.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined_result = {}\n",
    "    cleaned_text = clean_text_for_model(text_content)\n",
    "    report_ref_guess = find_report_reference(cleaned_text[:3000])\n",
    "    \n",
    "    # Get safe configuration\n",
    "    generation_config, safety_settings = get_safe_generation_config()\n",
    "    \n",
    "    # Initialize model with safety settings\n",
    "    # Using gemini-2.0-flash instead of 2.0 for better rate limits\n",
    "    model = genai.GenerativeModel(\n",
    "        \"models/gemini-2.0-flash\",  # Changed to 1.5 for better free tier limits\n",
    "        safety_settings=safety_settings\n",
    "    )\n",
    "    \n",
    "    # -----------------------------\n",
    "    # PASS 1: Core identifiers\n",
    "    # -----------------------------\n",
    "    prompt_ids = f\"\"\"\n",
    "You are performing a DOCUMENT TRANSCRIPTION task.\n",
    "Extract structured data EXACTLY as it appears in the source document.\n",
    "\n",
    "Extract these fields from the Kenyan valuation report:\n",
    "- report_reference (format: SOO/DOO/5310/1/25)\n",
    "- title_number\n",
    "- lr_number\n",
    "- ir_number\n",
    "- client_name\n",
    "- valuer_name\n",
    "- inspection_date (YYYY/MM/DD format)\n",
    "- valuation_date (YYYY/MM/DD format)\n",
    "\n",
    "Hint for report_reference: \"{report_ref_guess}\"\n",
    "\n",
    "Return ONLY this JSON structure:\n",
    "{{\n",
    "  \"property_id\": \"\",\n",
    "  \"report_reference\": \"\",\n",
    "  \"title_number\": \"\",\n",
    "  \"lr_number\": \"\",\n",
    "  \"ir_number\": \"\",\n",
    "  \"client_name\": \"\",\n",
    "  \"valuer_name\": \"\",\n",
    "  \"inspection_date\": \"\",\n",
    "  \"valuation_date\": \"\"\n",
    "}}\n",
    "\n",
    "SOURCE TEXT (first 5000 chars):\n",
    "{cleaned_text[:5000]}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"      â†’ Extracting IDs...\")\n",
    "        response = call_gemini_with_retry(model, prompt_ids, generation_config, safety_settings)\n",
    "        \n",
    "        if response:\n",
    "            ids_json = clean_gemini_json(response.text)\n",
    "            extracted_ids = json.loads(ids_json)\n",
    "            combined_result = deep_merge(combined_result, extracted_ids)\n",
    "            print(\"      âœ“ IDs extracted\")\n",
    "        else:\n",
    "            print(\"      âš  ID extraction failed\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"      âš  JSON parsing error in IDs: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      âš  Gemini ID extraction failed: {e}\")\n",
    "    \n",
    "    # Add delay between API calls to respect rate limits\n",
    "    sleep(2)  # 2 second delay between passes\n",
    "    \n",
    "    # -----------------------------\n",
    "    # PASS 2: Land, valuation, metadata\n",
    "    # -----------------------------\n",
    "    prompt_land = f\"\"\"\n",
    "You are performing a DOCUMENT TRANSCRIPTION task.\n",
    "Extract structured data EXACTLY as it appears in the source document.\n",
    "\n",
    "Extract these fields from the Kenyan valuation report:\n",
    "- location_county\n",
    "- location_description\n",
    "- location_coordinates\n",
    "- plot_area_hectares (number)\n",
    "- plot_area_acres (number)\n",
    "- land_use\n",
    "- plot_shape\n",
    "- soil_type\n",
    "- gradient\n",
    "- tenure_type\n",
    "- registered_proprietor\n",
    "- encumbrances\n",
    "- market_value_amount (number)\n",
    "- market_value_currency\n",
    "\n",
    "Return ONLY this JSON structure:\n",
    "{{\n",
    "  \"location_county\": \"\",\n",
    "  \"location_description\": \"\",\n",
    "  \"location_coordinates\": \"\",\n",
    "  \"plot_area_hectares\": 0,\n",
    "  \"plot_area_acres\": 0,\n",
    "  \"land_use\": \"\",\n",
    "  \"plot_shape\": \"\",\n",
    "  \"soil_type\": \"\",\n",
    "  \"gradient\": \"\",\n",
    "  \"drainage\": \"\",\n",
    "  \"vegetation\": \"\",\n",
    "  \"tenure_type\": \"\",\n",
    "  \"registered_proprietor\": \"\",\n",
    "  \"ownership_type\": \"\",\n",
    "  \"encumbrances\": \"\",\n",
    "  \"market_value_amount\": 0,\n",
    "  \"market_value_currency\": \"KShs\"\n",
    "}}\n",
    "\n",
    "SOURCE TEXT:\n",
    "{cleaned_text[:15000]}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"      â†’ Extracting land/valuation data...\")\n",
    "        response = call_gemini_with_retry(model, prompt_land, generation_config, safety_settings)\n",
    "        \n",
    "        if response:\n",
    "            land_json = clean_gemini_json(response.text)\n",
    "            extracted_land = json.loads(land_json)\n",
    "            combined_result = deep_merge(combined_result, extracted_land)\n",
    "            print(\"      âœ“ Land data extracted\")\n",
    "        else:\n",
    "            print(\"      âš  Land extraction failed\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"      âš  JSON parsing error in Land: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      âš  Gemini Land extraction failed: {e}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Final adjustments\n",
    "    # -----------------------------\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "    if \"report_reference\" not in combined_result or not combined_result[\"report_reference\"]:\n",
    "        combined_result[\"report_reference\"] = report_ref_guess\n",
    "    \n",
    "    # Metadata skeleton\n",
    "    if \"metadata\" not in combined_result:\n",
    "        combined_result[\"metadata\"] = {\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": 0,\n",
    "            \"processing_time_seconds\": 0,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": 0,\n",
    "            \"model_name\": \"gemini-1.5-flash\",\n",
    "            \"timestamp\": \"\"\n",
    "        }\n",
    "    \n",
    "    # Ensure string fields\n",
    "    string_fields = [\n",
    "        \"title_number\", \"lr_number\", \"ir_number\", \"client_name\",\n",
    "        \"valuer_name\", \"inspection_date\", \"valuation_date\"\n",
    "    ]\n",
    "    \n",
    "    for field in string_fields:\n",
    "        if field in combined_result and combined_result[field] is None:\n",
    "            combined_result[field] = \"\"\n",
    "    \n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        pages_processed = text_content.count('--- Page')  # approximate page count\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Inject real metadata\n",
    "        # -------------------------\n",
    "        file_size_kb = round(Path(pdf_path).stat().st_size / 1024, 2)\n",
    "        processing_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        if \"metadata\" not in extracted_data:\n",
    "            extracted_data[\"metadata\"] = {}\n",
    "        \n",
    "        extracted_data[\"metadata\"].update({\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined with full metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in PDF_DIR, generate OCR, send to Gemini, save JSON, and upload to Supabase.\n",
    "       Skips PDFs that already have a JSON output file.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Scan directories\n",
    "    # -------------------------\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    # Identify JSON files already processed\n",
    "    processed_json_stems = {p.stem for p in Path(OUTPUT_DIR).glob(\"*.json\")}\n",
    "\n",
    "    # Filter: process ONLY new PDFs\n",
    "    pdf_files = [p for p in pdf_files if p.stem not in processed_json_stems]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"ðŸŽ‰ All PDFs already processed â€” nothing to do.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} NEW PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "    # Process each PDF\n",
    "    # -------------------------\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        file_start_time = time.time()\n",
    "        file_size_kb = round(pdf_path.stat().st_size / 1024, 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. OCR extraction\n",
    "        # -------------------------\n",
    "        text_content = process_pdf_with_fallback(str(pdf_path))\n",
    "        pages_processed = text_content.count('--- Page')  # reliable heuristic\n",
    "        print(f\"  âœ“ OCR complete ({pages_processed} pages, {len(text_content)} chars)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Gemini extraction\n",
    "        # -------------------------\n",
    "        extracted = extract_with_gemini(text_content, pdf_path.name)\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Add metadata to JSON\n",
    "        # -------------------------\n",
    "        processing_time = round(time.time() - file_start_time, 2)\n",
    "        if \"metadata\" not in extracted:\n",
    "            extracted[\"metadata\"] = {}\n",
    "\n",
    "        extracted[\"metadata\"].update({\n",
    "            \"source_file\": pdf_path.name,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Save the JSON output\n",
    "        # -------------------------\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{pdf_path.stem}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ“ Saved â†’ {output_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Upload to Supabase\n",
    "        # -------------------------\n",
    "        if supabase_client:\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted, pdf_path.name)\n",
    "                print(f\"âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Supabase upload failed: {str(e)[:120]}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Track results\n",
    "        # -------------------------\n",
    "        results.append({\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"success\": bool(extracted),\n",
    "            \"data\": extracted,\n",
    "            \"timing\": {\"processing_seconds\": processing_time},\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Save batch summary\n",
    "    # -------------------------\n",
    "    total_time = round(time.time() - total_start, 2)\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Batch finished in {total_time/60:.1f} minutes\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "873507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n",
      "  Gemini Model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"property_id\", \"valuation_report\", \"property_details\", \"valuations\"],\n",
    "    \"properties\": {\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"valuation_report\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"report_reference\": {\"type\": \"string\"},\n",
    "                \"valuer\": {\"type\": \"string\"},\n",
    "                \"valuers\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"string\"},\n",
    "                            \"qualification\": {\"type\": \"string\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inspection_date\": {\"type\": \"string\"},\n",
    "                \"report_date\": {\"type\": \"string\"},\n",
    "                \"client\": {\"type\": \"string\"},\n",
    "                \"client_address\": {\"type\": \"string\"},\n",
    "                \"purpose\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_details\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"apartment_number\": {\"type\": \"string\"},\n",
    "                \"block\": {\"type\": \"string\"},\n",
    "                \"floor\": {\"type\": \"string\"},\n",
    "                \"title_details\": {\"type\": \"object\"},\n",
    "                \"location\": {\"type\": \"object\"},\n",
    "                \"tenure\": {\"type\": \"object\"},\n",
    "                \"registered_proprietors\": {\"type\": \"array\"},\n",
    "                \"ownership_type\": {\"type\": \"string\"},\n",
    "                \"encumbrances\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"property_description\": {\"type\": \"object\"},\n",
    "        \"apartment_details\": {\"type\": \"object\"},\n",
    "        \"occupancy\": {\"type\": \"string\"},\n",
    "        \"condition\": {\"type\": \"string\"},\n",
    "        \"market_assessment\": {\"type\": \"object\"},\n",
    "        \"valuations\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"current_market_value\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"amount\": {\"type\": \"number\"},\n",
    "                        \"currency\": {\"type\": \"string\"},\n",
    "                        \"amount_words\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"valuation_methodology\": {\"type\": \"array\"},\n",
    "        \"lease_details\": {\"type\": \"object\"},\n",
    "        \"compliance\": {\"type\": \"object\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ JSON schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase client initialized\n"
     ]
    }
   ],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase functions defined\n"
     ]
    }
   ],
   "source": [
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload to Supabase\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = data.copy()\n",
    "        upload_data['source_filename'] = filename\n",
    "        upload_data['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        client.table(\"property_valuations\").insert(upload_data).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Supabase functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=300, fmt='jpeg', thread_count=2)\n",
    "        \n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        # OCR each page\n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 1'\n",
    "            )\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ OCR function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72030385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gemini extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"Extract structured data using Google Gemini\"\"\"\n",
    "    import google.generativeai as genai\n",
    "    \n",
    "    if not GEMINI_API_KEY:\n",
    "        raise Exception(\"GEMINI_API_KEY not found\")\n",
    "    \n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    text_sample = text_content[:80000] if len(text_content) > 80000 else text_content\n",
    "    \n",
    "    print(f\"    â†’ Sending {len(text_sample)} chars to Gemini...\")\n",
    "    \n",
    "    prompt = f\"\"\"Extract property valuation data from this Kenyan document (OCR text). Return ONLY valid JSON.\n",
    "\n",
    "RULES:\n",
    "- Extract EXACT values from document (no placeholders like \"John Doe\")\n",
    "- If field not found, use \"\" or {{}}\n",
    "- Amounts as numbers without commas (8500000 not \"8,500,000\")\n",
    "- Dates as YYYY-MM-DD\n",
    "\n",
    "Schema:\n",
    "{{\n",
    "  \"property_id\": \"string\",\n",
    "  \"valuation_report\": {{\n",
    "    \"report_reference\": \"\", \"valuer\": \"\", \"valuers\": [], \"inspection_date\": \"\",\n",
    "    \"report_date\": \"\", \"client\": \"\", \"client_address\": \"\", \"purpose\": \"\"\n",
    "  }},\n",
    "  \"property_details\": {{\n",
    "    \"apartment_number\": \"\", \"block\": \"\", \"floor\": \"\",\n",
    "    \"title_details\": {{}}, \"location\": {{}}, \"tenure\": {{}},\n",
    "    \"registered_proprietors\": [], \"ownership_type\": \"\", \"encumbrances\": \"\"\n",
    "  }},\n",
    "  \"property_description\": {{}},\n",
    "  \"apartment_details\": {{}},\n",
    "  \"occupancy\": \"\",\n",
    "  \"condition\": \"\",\n",
    "  \"market_assessment\": {{}},\n",
    "  \"valuations\": {{\n",
    "    \"current_market_value\": {{\"amount\": 0, \"currency\": \"KES\", \"amount_words\": \"\"}}\n",
    "  }},\n",
    "  \"valuation_methodology\": [],\n",
    "  \"lease_details\": {{}},\n",
    "  \"compliance\": {{}}\n",
    "}}\n",
    "\n",
    "Document:\n",
    "{text_sample}\n",
    "\n",
    "Return ONLY JSON:\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=4096,\n",
    "            )\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        print(f\"    â†’ Gemini responded in {elapsed:.1f}s\")\n",
    "        \n",
    "        # Save response\n",
    "        debug_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_gemini_response.txt\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response_text)\n",
    "        \n",
    "        # Clean JSON\n",
    "        if '```json' in response_text:\n",
    "            response_text = response_text.split('```json')[1].split('```')[0]\n",
    "        elif '```' in response_text:\n",
    "            response_text = response_text.split('```')[1].split('```')[0]\n",
    "        \n",
    "        response_text = response_text.strip()\n",
    "        \n",
    "        if not response_text.startswith('{'):\n",
    "            start = response_text.find('{')\n",
    "            end = response_text.rfind('}')\n",
    "            if start != -1 and end != -1:\n",
    "                response_text = response_text[start:end+1]\n",
    "        \n",
    "        extracted_data = json.loads(response_text)\n",
    "        \n",
    "        if not extracted_data.get('property_id'):\n",
    "            extracted_data['property_id'] = Path(filename).stem\n",
    "        \n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        raise Exception(f\"JSON parsing failed: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Gemini error: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Gemini extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Batch processing defined\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs in {PDF_DIR}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    for idx, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_file.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        result = process_single_pdf_complete(str(pdf_file), pdf_file.name)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"\\nâœ… SUCCESS - {result['timing']['total']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ FAILED\")\n",
    "    \n",
    "    # Summary\n",
    "    total_time = time.time() - total_start\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“Š SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total: {len(results)} | Success: {successful} | Failed: {len(results)-successful}\")\n",
    "    print(f\"Time: {total_time/60:.1f} min | Avg: {total_time/len(results):.1f}s/PDF\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': GEMINI_MODEL,\n",
    "            'total': len(results),\n",
    "            'successful': successful,\n",
    "            'results': results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Batch processing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª TESTING: doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf\n",
      "\n",
      "\n",
      "  ðŸ“„ Stage 1: OCR\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 12 pages...\n",
      "      â†’ Page 1/12... âœ“ 1266 chars\n",
      "      â†’ Page 2/12... âœ“ 2401 chars\n",
      "      â†’ Page 3/12... âœ“ 165 chars\n",
      "      â†’ Page 4/12... âœ“ 182 chars\n",
      "      â†’ Page 5/12... âœ“ 1357 chars\n",
      "      â†’ Page 6/12... âœ“ 1800 chars\n",
      "      â†’ Page 7/12... âœ“ 1664 chars\n",
      "      â†’ Page 8/12... âœ“ 539 chars\n",
      "      â†’ Page 9/12... âœ“ 491 chars\n",
      "      â†’ Page 10/12... âœ“ 581 chars\n",
      "      â†’ Page 11/12... âœ“ 1039 chars\n",
      "      â†’ Page 12/12... âœ“ 607 chars\n",
      "    â†’ OCR complete: 12297 characters\n",
      "  âœ“ 12297 chars in 50.5s\n",
      "\n",
      "  ðŸ¤– Stage 2: Gemini\n",
      "    â†’ Sending 12297 chars to Gemini...\n",
      "\n",
      "  âœ— Failed: Gemini error: Invalid operation: The `response.text` quick accessor requires the response to contain\n",
      "\n",
      "âŒ TEST FAILED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': 'doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf',\n",
       " 'success': False,\n",
       " 'stage': 'gemini',\n",
       " 'error': \"Gemini error: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.\",\n",
       " 'data': None,\n",
       " 'timing': {'ocr': 50.51800060272217, 'total': 68.88411569595337}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "179daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = process_all_pdfs_complete()\n",
    "# results\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa54f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

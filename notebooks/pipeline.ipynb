{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4281297",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Year 2025\\Year 2025\\Additional\"\n",
    "OUTPUT_DIR = \"./additional_2025v1\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "# print(f\"Key: {SUPABASE_KEY}, \\n URL: {SUPABASE_URL}\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"property_id\",\n",
    "        \"report_reference\",\n",
    "        \"market_value_amount\",\n",
    "        \"metadata\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "\n",
    "        # --- CORE IDENTIFIERS ---\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"report_reference\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- TITLE INFORMATION ---\n",
    "        \"title_number\": {\"type\": \"string\"},\n",
    "        \"lr_number\": {\"type\": \"string\"},\n",
    "        \"ir_number\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- CLIENT + VALUER ---\n",
    "        \"client_name\": {\"type\": \"string\"},\n",
    "        \"valuer_name\": {\"type\": \"string\"},\n",
    "        \"inspection_date\": {\"type\": \"string\"},\n",
    "        \"valuation_date\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- LOCATION ---\n",
    "        \"location_county\": {\"type\": \"string\"},\n",
    "        \"location_description\": {\"type\": \"string\"},\n",
    "        \"location_coordinates\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- PARCEL DETAILS ---\n",
    "        \"plot_area_hectares\": {\"type\": \"number\"},\n",
    "        \"plot_area_acres\": {\"type\": \"number\"},\n",
    "        \"land_use\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- NEW LAND PHYSICAL FEATURES ---\n",
    "        \"plot_shape\": {\"type\": \"string\"},     # rectangular, irregular, square\n",
    "        \"soil_type\": {\"type\": \"string\"},      # black cotton, red soil, sandy, loam, murram\n",
    "        \"gradient\": {\"type\": \"string\"},       # flat, gentle slope, moderate slope, steep\n",
    "        \"drainage\": {\"type\": \"string\"},       # optional recommendation\n",
    "        \"vegetation\": {\"type\": \"string\"},     # optional recommendation\n",
    "\n",
    "        # --- TENURE + OWNERSHIP ---\n",
    "        \"tenure_type\": {\"type\": \"string\"},\n",
    "        \"registered_proprietor\": {\"type\": \"string\"},\n",
    "        \"ownership_type\": {\"type\": \"string\"},\n",
    "        \"encumbrances\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- VALUATION ---\n",
    "        \"market_value_amount\": {\"type\": \"number\"},\n",
    "        \"market_value_currency\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- METADATA ---\n",
    "        \"metadata\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"source_file\": {\"type\": \"string\"},\n",
    "                \"file_size_kb\": {\"type\": \"number\"},\n",
    "                \"processing_time_seconds\": {\"type\": \"number\"},\n",
    "                \"ocr_used\": {\"type\": \"boolean\"},\n",
    "                \"pages_processed\": {\"type\": \"number\"},\n",
    "                \"model_name\": {\"type\": \"string\"},\n",
    "                \"timestamp\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Updated JSON schema defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_TABLE = \"additional_2025v1\"   \n",
    "\n",
    "\n",
    "def sanitize_dates_for_supabase(data: dict) -> dict:\n",
    "    \"\"\"Convert empty string dates to None for Supabase insert\"\"\"\n",
    "    date_fields = [\"inspection_date\", \"valuation_date\"]\n",
    "    for field in date_fields:\n",
    "        if field in data and (data[field] == \"\" or data[field] is None):\n",
    "            data[field] = None\n",
    "    return data\n",
    "\n",
    "\n",
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload extracted valuation data to Supabase (restricted to valid columns).\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = sanitize_dates_for_supabase(data.copy())\n",
    "\n",
    "        # Allowed columns EXACTLY as in your Supabase table\n",
    "        allowed_columns = {\n",
    "            \"property_id\",\n",
    "            \"report_reference\",\n",
    "            \"title_number\",\n",
    "            \"lr_number\",\n",
    "            \"ir_number\",\n",
    "            \"client_name\",\n",
    "            \"valuer_name\",\n",
    "            \"inspection_date\",\n",
    "            \"valuation_date\",\n",
    "            \"location_county\",\n",
    "            \"location_description\",\n",
    "            \"location_coordinates\",\n",
    "            \"plot_area_hectares\",\n",
    "            \"plot_area_acres\",\n",
    "            \"land_use\",\n",
    "            \"plot_shape\",\n",
    "            \"soil_type\",\n",
    "            \"gradient\",\n",
    "            \"drainage\",\n",
    "            \"vegetation\",\n",
    "            \"tenure_type\",\n",
    "            \"registered_proprietor\",\n",
    "            \"ownership_type\",\n",
    "            \"encumbrances\",\n",
    "            \"market_value_amount\",\n",
    "            \"market_value_currency\",\n",
    "            \"metadata\"\n",
    "        }\n",
    "\n",
    "        # Remove any field not in table\n",
    "        clean_record = {k: v for k, v in upload_data.items() if k in allowed_columns}\n",
    "\n",
    "        # Insert into your table\n",
    "        client.table(SUPABASE_TABLE).insert(clean_record).execute()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str, dpi: int = 150) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR safely (memory-friendly).\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        import gc\n",
    "\n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images at {dpi} dpi...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, fmt='jpeg', thread_count=2)\n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            # OCR the page (use psm 6 for memory efficiency)\n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 6'\n",
    "            ).strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "            \n",
    "            # Free memory after each page\n",
    "            image.close()\n",
    "            del image\n",
    "            gc.collect()\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output for debugging\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Updated OCR function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd186301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def clean_text_for_model(text: str) -> str:\n",
    "    \"\"\"Remove page headers, footers, and empty lines\"\"\"\n",
    "    # Remove page markers like --- Page 3 ---\n",
    "    text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "    # Remove multiple empty lines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_report_reference(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect report reference numbers like:\n",
    "    SOO/DOO/5310/1/25, SOO/5307/1/25, DOO/5307/1/25, VP/1122/24\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b(?:[A-Z]{2,3}/)?(?:[A-Z]{2,3}/)?\\d{3,5}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        # Choose the most complete / longest match\n",
    "        return max(matches, key=len)\n",
    "    return \"\"\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}')\n",
    "    if start != -1 and end != -1:\n",
    "        response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Merge dictionaries recursively\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction function\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract structured property valuation data from a Kenyan PDF OCR text.\n",
    "    Two-pass approach: first IDs, then land/valuation.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_result = {}\n",
    "    cleaned_text = clean_text_for_model(text_content)\n",
    "    report_ref_guess = find_report_reference(cleaned_text[:3000])  # first 3k chars\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 1: Core identifiers\n",
    "    # -----------------------------\n",
    "    prompt_ids = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- report_reference (like SOO/DOO/5310/1/25)\n",
    "- title_number\n",
    "- lr_number\n",
    "- ir_number\n",
    "- client_name\n",
    "- valuer_name\n",
    "- inspection_date\n",
    "- valuation_date\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as empty string \"\".\n",
    "If multiple report_reference exist, choose the longest one.\n",
    "Use this as a hint for report_reference if present: \"{report_ref_guess}\"\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"property_id\": \"\",\n",
    "  \"report_reference\": \"\",\n",
    "  \"title_number\": \"\",\n",
    "  \"lr_number\": \"\",\n",
    "  \"ir_number\": \"\",\n",
    "  \"client_name\": \"\",\n",
    "  \"valuer_name\": \"\",\n",
    "  \"inspection_date\": \"\",\n",
    "  \"valuation_date\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text[:5000]}\n",
    "\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            prompt_ids,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=2048)\n",
    "        )\n",
    "        ids_json = clean_gemini_json(response.text)\n",
    "        extracted_ids = json.loads(ids_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini ID extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 2: Land, valuation, metadata\n",
    "    # -----------------------------\n",
    "    prompt_land = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- location_county\n",
    "- location_description\n",
    "- location_coordinates\n",
    "- plot_area_hectares\n",
    "- plot_area_acres\n",
    "- land_use\n",
    "- plot_shape\n",
    "- soil_type\n",
    "- gradient\n",
    "- drainage\n",
    "- vegetation\n",
    "- tenure_type\n",
    "- registered_proprietor\n",
    "- ownership_type\n",
    "- encumbrances\n",
    "- market_value_amount\n",
    "- market_value_currency\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as \"\" for strings and 0 for numbers.\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"location_county\": \"\",\n",
    "  \"location_description\": \"\",\n",
    "  \"location_coordinates\": \"\",\n",
    "  \"plot_area_hectares\": 0,\n",
    "  \"plot_area_acres\": 0,\n",
    "  \"land_use\": \"\",\n",
    "  \"plot_shape\": \"\",\n",
    "  \"soil_type\": \"\",\n",
    "  \"gradient\": \"\",\n",
    "  \"drainage\": \"\",\n",
    "  \"vegetation\": \"\",\n",
    "  \"tenure_type\": \"\",\n",
    "  \"registered_proprietor\": \"\",\n",
    "  \"ownership_type\": \"\",\n",
    "  \"encumbrances\": \"\",\n",
    "  \"market_value_amount\": 0,\n",
    "  \"market_value_currency\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt_land,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4096)\n",
    "        )\n",
    "        land_json = clean_gemini_json(response.text)\n",
    "        extracted_land = json.loads(land_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_land)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini Land extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final adjustments\n",
    "    # -----------------------------\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "    if \"report_reference\" not in combined_result or not combined_result[\"report_reference\"]:\n",
    "        combined_result[\"report_reference\"] = report_ref_guess\n",
    "\n",
    "    # Metadata skeleton (to be populated later)\n",
    "    if \"metadata\" not in combined_result:\n",
    "        combined_result[\"metadata\"] = {\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": 0,\n",
    "            \"processing_time_seconds\": 0,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": 0,\n",
    "            \"model_name\": \"gemini-2.5-flash\",\n",
    "            \"timestamp\": \"\"\n",
    "        }\n",
    "    # Ensure ID/date fields stay as strings\n",
    "    string_fields = [\n",
    "        \"title_number\",\n",
    "        \"lr_number\",\n",
    "        \"ir_number\",\n",
    "        \"client_name\",\n",
    "        \"valuer_name\",\n",
    "        \"inspection_date\",\n",
    "        \"valuation_date\",\n",
    "    ]\n",
    "\n",
    "    for field in string_fields:\n",
    "        if field in combined_result and combined_result[field] is None:\n",
    "            combined_result[field] = \"\"\n",
    "\n",
    "\n",
    "    return combined_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        pages_processed = text_content.count('--- Page')  # approximate page count\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Inject real metadata\n",
    "        # -------------------------\n",
    "        file_size_kb = round(Path(pdf_path).stat().st_size / 1024, 2)\n",
    "        processing_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        if \"metadata\" not in extracted_data:\n",
    "            extracted_data[\"metadata\"] = {}\n",
    "        \n",
    "        extracted_data[\"metadata\"].update({\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined with full metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in PDF_DIR, generate OCR, send to Gemini, save JSON, and upload to Supabase.\n",
    "       Skips PDFs that already have a JSON output file.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Scan directories\n",
    "    # -------------------------\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    # Identify JSON files already processed\n",
    "    processed_json_stems = {p.stem for p in Path(OUTPUT_DIR).glob(\"*.json\")}\n",
    "\n",
    "    # Filter: process ONLY new PDFs\n",
    "    pdf_files = [p for p in pdf_files if p.stem not in processed_json_stems]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"ðŸŽ‰ All PDFs already processed â€” nothing to do.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} NEW PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "    # Process each PDF\n",
    "    # -------------------------\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        file_start_time = time.time()\n",
    "        file_size_kb = round(pdf_path.stat().st_size / 1024, 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. OCR extraction\n",
    "        # -------------------------\n",
    "        text_content = process_pdf_with_fallback(str(pdf_path))\n",
    "        pages_processed = text_content.count('--- Page')  # reliable heuristic\n",
    "        print(f\"  âœ“ OCR complete ({pages_processed} pages, {len(text_content)} chars)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Gemini extraction\n",
    "        # -------------------------\n",
    "        extracted = extract_with_gemini(text_content, pdf_path.name)\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Add metadata to JSON\n",
    "        # -------------------------\n",
    "        processing_time = round(time.time() - file_start_time, 2)\n",
    "        if \"metadata\" not in extracted:\n",
    "            extracted[\"metadata\"] = {}\n",
    "\n",
    "        extracted[\"metadata\"].update({\n",
    "            \"source_file\": pdf_path.name,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Save the JSON output\n",
    "        # -------------------------\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{pdf_path.stem}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ“ Saved â†’ {output_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Upload to Supabase\n",
    "        # -------------------------\n",
    "        if supabase_client:\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted, pdf_path.name)\n",
    "                print(f\"âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Supabase upload failed: {str(e)[:120]}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Track results\n",
    "        # -------------------------\n",
    "        results.append({\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"success\": bool(extracted),\n",
    "            \"data\": extracted,\n",
    "            \"timing\": {\"processing_seconds\": processing_time},\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Save batch summary\n",
    "    # -------------------------\n",
    "    total_time = round(time.time() - total_start, 2)\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Batch finished in {total_time/60:.1f} minutes\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

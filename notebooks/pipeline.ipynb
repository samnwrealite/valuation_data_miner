{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\documents\\work\\code\\valuation_data_miner\\env\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\n",
      "  Gemini Model: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\data\"\n",
    "OUTPUT_DIR = \"./extracted_data_v2\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"test\": 123}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"property_id\",\n",
    "        \"report_reference\",\n",
    "        \"market_value_amount\",\n",
    "        \"metadata\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "\n",
    "        # --- CORE IDENTIFIERS ---\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"report_reference\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- TITLE INFORMATION ---\n",
    "        \"title_number\": {\"type\": \"string\"},\n",
    "        \"lr_number\": {\"type\": \"string\"},\n",
    "        \"ir_number\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- CLIENT + VALUER ---\n",
    "        \"client_name\": {\"type\": \"string\"},\n",
    "        \"valuer_name\": {\"type\": \"string\"},\n",
    "        \"inspection_date\": {\"type\": \"string\"},\n",
    "        \"report_date\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- LOCATION ---\n",
    "        \"location_county\": {\"type\": \"string\"},\n",
    "        \"location_description\": {\"type\": \"string\"},\n",
    "        \"location_coordinates\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- PARCEL DETAILS ---\n",
    "        \"plot_area_hectares\": {\"type\": \"number\"},\n",
    "        \"plot_area_acres\": {\"type\": \"number\"},\n",
    "        \"land_use\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- NEW LAND PHYSICAL FEATURES ---\n",
    "        \"plot_shape\": {\"type\": \"string\"},     # rectangular, irregular, square\n",
    "        \"soil_type\": {\"type\": \"string\"},      # black cotton, red soil, sandy, loam, murram\n",
    "        \"gradient\": {\"type\": \"string\"},       # flat, gentle slope, moderate slope, steep\n",
    "        \"drainage\": {\"type\": \"string\"},       # optional recommendation\n",
    "        \"vegetation\": {\"type\": \"string\"},     # optional recommendation\n",
    "\n",
    "        # --- TENURE + OWNERSHIP ---\n",
    "        \"tenure_type\": {\"type\": \"string\"},\n",
    "        \"registered_proprietor\": {\"type\": \"string\"},\n",
    "        \"ownership_type\": {\"type\": \"string\"},\n",
    "        \"encumbrances\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- VALUATION ---\n",
    "        \"market_value_amount\": {\"type\": \"number\"},\n",
    "        \"market_value_currency\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- METADATA ---\n",
    "        \"metadata\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"source_file\": {\"type\": \"string\"},\n",
    "                \"file_size_kb\": {\"type\": \"number\"},\n",
    "                \"processing_time_seconds\": {\"type\": \"number\"},\n",
    "                \"ocr_used\": {\"type\": \"boolean\"},\n",
    "                \"pages_processed\": {\"type\": \"number\"},\n",
    "                \"model_name\": {\"type\": \"string\"},\n",
    "                \"timestamp\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Updated JSON schema defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase client initialized\n"
     ]
    }
   ],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_dates_for_supabase(data: dict) -> dict:\n",
    "    \"\"\"Convert empty string dates to None for Supabase insert\"\"\"\n",
    "    date_fields = [\"inspection_date\", \"report_date\"]\n",
    "    for field in date_fields:\n",
    "        if field in data and (data[field] == \"\" or data[field] is None):\n",
    "            data[field] = None\n",
    "    return data\n",
    "\n",
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload to Supabase\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = data.copy()\n",
    "        upload_data = sanitize_dates_for_supabase(upload_data)\n",
    "\n",
    "        # Only include columns in your table\n",
    "        for extra in [\"source_filename\", \"processed_at\"]:\n",
    "            upload_data.pop(extra, None)\n",
    "\n",
    "        client.table(\"property_valuations\").insert(upload_data).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=300, fmt='jpeg', thread_count=2)\n",
    "        \n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        # OCR each page\n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 1'\n",
    "            )\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ OCR function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def clean_text_for_model(text: str) -> str:\n",
    "    \"\"\"Remove page headers, footers, and empty lines\"\"\"\n",
    "    # Remove page markers like --- Page 3 ---\n",
    "    text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "    # Remove multiple empty lines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_report_reference(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect report reference numbers like:\n",
    "    SOO/DOO/5310/1/25, SOO/5307/1/25, DOO/5307/1/25, VP/1122/24\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b(?:[A-Z]{2,3}/)?(?:[A-Z]{2,3}/)?\\d{3,5}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        # Choose the most complete / longest match\n",
    "        return max(matches, key=len)\n",
    "    return \"\"\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}')\n",
    "    if start != -1 and end != -1:\n",
    "        response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Merge dictionaries recursively\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction function\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract structured property valuation data from a Kenyan PDF OCR text.\n",
    "    Two-pass approach: first IDs, then land/valuation.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_result = {}\n",
    "    cleaned_text = clean_text_for_model(text_content)\n",
    "    report_ref_guess = find_report_reference(cleaned_text[:3000])  # first 3k chars\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 1: Core identifiers\n",
    "    # -----------------------------\n",
    "    prompt_ids = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- report_reference (like SOO/DOO/5310/1/25)\n",
    "- title_number\n",
    "- lr_number\n",
    "- ir_number\n",
    "- client_name\n",
    "- valuer_name\n",
    "- inspection_date\n",
    "- report_date\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as empty string \"\".\n",
    "If multiple report_reference exist, choose the longest one.\n",
    "Use this as a hint for report_reference if present: \"{report_ref_guess}\"\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"property_id\": \"\",\n",
    "  \"report_reference\": \"\",\n",
    "  \"title_number\": \"\",\n",
    "  \"lr_number\": \"\",\n",
    "  \"ir_number\": \"\",\n",
    "  \"client_name\": \"\",\n",
    "  \"valuer_name\": \"\",\n",
    "  \"inspection_date\": \"\",\n",
    "  \"report_date\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text[:5000]}\n",
    "\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            prompt_ids,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=2048)\n",
    "        )\n",
    "        ids_json = clean_gemini_json(response.text)\n",
    "        extracted_ids = json.loads(ids_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini ID extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 2: Land, valuation, metadata\n",
    "    # -----------------------------\n",
    "    prompt_land = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- location_county\n",
    "- location_description\n",
    "- location_coordinates\n",
    "- plot_area_hectares\n",
    "- plot_area_acres\n",
    "- land_use\n",
    "- plot_shape\n",
    "- soil_type\n",
    "- gradient\n",
    "- drainage\n",
    "- vegetation\n",
    "- tenure_type\n",
    "- registered_proprietor\n",
    "- ownership_type\n",
    "- encumbrances\n",
    "- market_value_amount\n",
    "- market_value_currency\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as \"\" for strings and 0 for numbers.\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"location_county\": \"\",\n",
    "  \"location_description\": \"\",\n",
    "  \"location_coordinates\": \"\",\n",
    "  \"plot_area_hectares\": 0,\n",
    "  \"plot_area_acres\": 0,\n",
    "  \"land_use\": \"\",\n",
    "  \"plot_shape\": \"\",\n",
    "  \"soil_type\": \"\",\n",
    "  \"gradient\": \"\",\n",
    "  \"drainage\": \"\",\n",
    "  \"vegetation\": \"\",\n",
    "  \"tenure_type\": \"\",\n",
    "  \"registered_proprietor\": \"\",\n",
    "  \"ownership_type\": \"\",\n",
    "  \"encumbrances\": \"\",\n",
    "  \"market_value_amount\": 0,\n",
    "  \"market_value_currency\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt_land,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4096)\n",
    "        )\n",
    "        land_json = clean_gemini_json(response.text)\n",
    "        extracted_land = json.loads(land_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_land)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini Land extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final adjustments\n",
    "    # -----------------------------\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "    if \"report_reference\" not in combined_result or not combined_result[\"report_reference\"]:\n",
    "        combined_result[\"report_reference\"] = report_ref_guess\n",
    "\n",
    "    # Metadata skeleton (to be populated later)\n",
    "    if \"metadata\" not in combined_result:\n",
    "        combined_result[\"metadata\"] = {\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": 0,\n",
    "            \"processing_time_seconds\": 0,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": 0,\n",
    "            \"model_name\": \"gemini-2.5-flash\",\n",
    "            \"timestamp\": \"\"\n",
    "        }\n",
    "\n",
    "    return combined_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined with full metadata\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        pages_processed = text_content.count('--- Page')  # approximate page count\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Inject real metadata\n",
    "        # -------------------------\n",
    "        file_size_kb = round(Path(pdf_path).stat().st_size / 1024, 2)\n",
    "        processing_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        if \"metadata\" not in extracted_data:\n",
    "            extracted_data[\"metadata\"] = {}\n",
    "        \n",
    "        extracted_data[\"metadata\"].update({\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined with full metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in the PDF_DIR, extract OCR, send to Gemini, and save JSON with full metadata.\"\"\"\n",
    "    \n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        file_start_time = time.time()\n",
    "        file_size_kb = round(pdf_path.stat().st_size / 1024, 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. OCR extraction\n",
    "        # -------------------------\n",
    "        text_content = process_pdf_with_fallback(str(pdf_path))\n",
    "        pages_processed = text_content.count('--- Page')  # reliable page count from OCR text\n",
    "        print(f\"  âœ“ OCR complete ({pages_processed} pages, {len(text_content)} chars)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Gemini extraction\n",
    "        # -------------------------\n",
    "        extracted = extract_with_gemini(text_content, pdf_path.name)\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Inject metadata block\n",
    "        # -------------------------\n",
    "        processing_time = round(time.time() - file_start_time, 2)\n",
    "        if \"metadata\" not in extracted:\n",
    "            extracted[\"metadata\"] = {}\n",
    "\n",
    "        extracted[\"metadata\"].update({\n",
    "            \"source_file\": pdf_path.name,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Save JSON output\n",
    "        # -------------------------\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{pdf_path.stem}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ“ Saved â†’ {output_path}\")\n",
    "\n",
    "        if supabase_client:\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted, pdf_path.name)\n",
    "                print(f\"âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Supabase upload failed: {str(e)[:100]}\")\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"success\": bool(extracted),\n",
    "            \"data\": extracted,\n",
    "            \"timing\": {\"processing_seconds\": processing_time},\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Batch summary file\n",
    "    # -------------------------\n",
    "    total_time = round(time.time() - total_start, 2)\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Batch finished in {total_time/60:.1f} minutes\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª TESTING: doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf\n",
      "\n",
      "\n",
      "  ðŸ“„ Stage 1: OCR\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 12 pages...\n",
      "      â†’ Page 1/12... âœ“ 1266 chars\n",
      "      â†’ Page 2/12... âœ“ 2401 chars\n",
      "      â†’ Page 3/12... âœ“ 165 chars\n",
      "      â†’ Page 4/12... âœ“ 182 chars\n",
      "      â†’ Page 5/12... âœ“ 1357 chars\n",
      "      â†’ Page 6/12... âœ“ 1800 chars\n",
      "      â†’ Page 7/12... âœ“ 1664 chars\n",
      "      â†’ Page 8/12... âœ“ 539 chars\n",
      "      â†’ Page 9/12... âœ“ 491 chars\n",
      "      â†’ Page 10/12... âœ“ 581 chars\n",
      "      â†’ Page 11/12... âœ“ 1039 chars\n",
      "      â†’ Page 12/12... âœ“ 607 chars\n",
      "    â†’ OCR complete: 12297 characters\n",
      "  âœ“ 12297 chars in 54.2s\n",
      "\n",
      "  ðŸ¤– Stage 2: Gemini\n",
      "âš  Gemini ID extraction failed: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.\n",
      "  âœ“ Extracted in 21.4s\n",
      "\n",
      "  âœ… Stage 3: Validation\n",
      "  âœ“ Valid\n",
      "\n",
      "  ðŸ’¾ Stage 4: Save\n",
      "  âœ“ Saved to doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.json\n",
      "  âš  Supabase: Supabase upload failed: {'message': \"Could not fin\n",
      "\n",
      "âœ… TEST PASSED!\n",
      "\n",
      "Preview:\n",
      "{\n",
      "  \"location_county\": \"Machakos County\",\n",
      "  \"location_description\": \"The property is situated approximately 700 meters off Mombasa Road, 2.3 kilometres to the East of Greenpark Estate and 2.5 kilometres due South of Lukenya Gateway within Lukenya Atea of Machakos County.\",\n",
      "  \"location_coordinates\": \"1\\u00b028'12.2\\\"S 37\\u00b002'21.7\\\"E\",\n",
      "  \"plot_area_hectares\": 2.024,\n",
      "  \"plot_area_acres\": 5.001,\n",
      "  \"land_use\": \"Mixed use\",\n",
      "  \"plot_shape\": \"rectangular\",\n",
      "  \"soil_type\": \"black cotton soils\",\n",
      "  \"gradient\": \"fairly level\",\n",
      "  \"drainage\": \"into a septic tank\",\n",
      "  \"vegetation\": \"\",\n",
      "  \"tenure_type\": \"leasehold interest\",\n",
      "  \"registered_proprietor\": \"Kenneth Wathome Mwatu\",\n",
      "  \"ownership_type\": \"\",\n",
      "  \"encumbrances\": \"Unascertained\",\n",
      "  \"market_value_amount\": 60000000,\n",
      "  \"market_value_currency\": \"KShs.\",\n",
      "  \"property_id\": \"doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY\",\n",
      "  \"report_reference\": \"SOO/DOO/5296/1/25\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"doc LR NO 7815-83 LUKENYA AREA OFF M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': 'doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf',\n",
       " 'success': True,\n",
       " 'stage': 'supabase',\n",
       " 'error': None,\n",
       " 'data': {'location_county': 'Machakos County',\n",
       "  'location_description': 'The property is situated approximately 700 meters off Mombasa Road, 2.3 kilometres to the East of Greenpark Estate and 2.5 kilometres due South of Lukenya Gateway within Lukenya Atea of Machakos County.',\n",
       "  'location_coordinates': '1Â°28\\'12.2\"S 37Â°02\\'21.7\"E',\n",
       "  'plot_area_hectares': 2.024,\n",
       "  'plot_area_acres': 5.001,\n",
       "  'land_use': 'Mixed use',\n",
       "  'plot_shape': 'rectangular',\n",
       "  'soil_type': 'black cotton soils',\n",
       "  'gradient': 'fairly level',\n",
       "  'drainage': 'into a septic tank',\n",
       "  'vegetation': '',\n",
       "  'tenure_type': 'leasehold interest',\n",
       "  'registered_proprietor': 'Kenneth Wathome Mwatu',\n",
       "  'ownership_type': '',\n",
       "  'encumbrances': 'Unascertained',\n",
       "  'market_value_amount': 60000000,\n",
       "  'market_value_currency': 'KShs.',\n",
       "  'property_id': 'doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY',\n",
       "  'report_reference': 'SOO/DOO/5296/1/25',\n",
       "  'metadata': {'source_file': 'doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf',\n",
       "   'file_size_kb': 6867.22,\n",
       "   'processing_time_seconds': 75.56,\n",
       "   'ocr_used': True,\n",
       "   'pages_processed': 12,\n",
       "   'model_name': 'models/gemini-2.5-flash',\n",
       "   'timestamp': '2025-11-24T16:30:36.047972'}},\n",
       " 'timing': {'ocr': 54.154062271118164,\n",
       "  'gemini': 21.406218767166138,\n",
       "  'total': 76.93905472755432}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ PROCESSING 47 PDFs\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "[1/47] doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 12 pages...\n",
      "      â†’ Page 1/12... âœ“ 1266 chars\n",
      "      â†’ Page 2/12... âœ“ 2401 chars\n",
      "      â†’ Page 3/12... âœ“ 165 chars\n",
      "      â†’ Page 4/12... âœ“ 182 chars\n",
      "      â†’ Page 5/12... âœ“ 1357 chars\n",
      "      â†’ Page 6/12... âœ“ 1800 chars\n",
      "      â†’ Page 7/12... âœ“ 1664 chars\n",
      "      â†’ Page 8/12... âœ“ 539 chars\n",
      "      â†’ Page 9/12... âœ“ 491 chars\n",
      "      â†’ Page 10/12... âœ“ 581 chars\n",
      "      â†’ Page 11/12... âœ“ 1039 chars\n",
      "      â†’ Page 12/12... âœ“ 607 chars\n",
      "    â†’ OCR complete: 12297 characters\n",
      "  âœ“ OCR complete (12 pages, 12297 chars)\n",
      "âœ“ Saved â†’ ./extracted_data_v2\\doc LR NO 7815-83 LUKENYA AREA OFF MOMBASA ROAD MACHAKOS COUNTY.json\n",
      "âœ“ Uploaded to Supabase\n",
      "\n",
      "======================================================================\n",
      "[2/47] doc39404020250117161224 (1) TITLE NO KARAI-GIKAMBURA-6448 ONDIRI AREA KIAMBU COUNTY.pdf\n",
      "======================================================================\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images...\n",
      "    â†’ Running OCR on 21 pages...\n",
      "      â†’ Page 1/21... âœ“ 1370 chars\n",
      "      â†’ Page 2/21... âœ“ 2369 chars\n",
      "      â†’ Page 3/21... âœ“ 158 chars\n",
      "      â†’ Page 4/21... âœ“ 129 chars\n",
      "      â†’ Page 5/21... âœ“ 259 chars\n",
      "      â†’ Page 6/21... âœ“ 1294 chars\n",
      "      â†’ Page 7/21... âœ“ 1411 chars\n",
      "      â†’ Page 8/21... âœ“ 1053 chars\n",
      "      â†’ Page 9/21... âœ“ 840 chars\n",
      "      â†’ Page 10/21... âœ“ 1770 chars\n",
      "      â†’ Page 11/21... âœ“ 1817 chars\n",
      "      â†’ Page 12/21... âœ“ 890 chars\n",
      "      â†’ Page 13/21... âœ“ 623 chars\n",
      "      â†’ Page 14/21... âœ“ 1014 chars\n",
      "      â†’ Page 15/21... âœ“ 129 chars\n",
      "      â†’ Page 16/21... âœ“ 1219 chars\n",
      "      â†’ Page 17/21... âœ“ 669 chars\n",
      "      â†’ Page 18/21... âœ“ 744 chars\n",
      "      â†’ Page 19/21... âœ“ 280 chars\n",
      "      â†’ Page 20/21... âœ“ 172 chars\n",
      "      â†’ Page 21/21... âœ“ 1957 chars\n",
      "    â†’ OCR complete: 20534 characters\n",
      "  âœ“ OCR complete (21 pages, 20534 chars)\n"
     ]
    }
   ],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

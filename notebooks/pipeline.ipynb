{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai pdf2image pytesseract pillow pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0761df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from jsonschema import validate, ValidationError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4281297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  PDF Directory: Z:\\Risper M\\ALL VALUATION REPORTS\n",
      "  Gemini Model: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r\"Z:\\Risper M\\ALL VALUATION REPORTS\"\n",
    "OUTPUT_DIR = \"./additional_2025v1\"\n",
    "ERROR_LOG_DIR = \"./errors\"\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = r\"C:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# API Keys\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "# print(f\"Key: {SUPABASE_KEY}, \\n URL: {SUPABASE_URL}\")\n",
    "\n",
    "# Model settings\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\" \n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERROR_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Gemini Model: {GEMINI_MODEL}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nâš  WARNING: GEMINI_API_KEY not found!\")\n",
    "    print(\"  Get key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89e9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Documents\\Work\\Code\\valuation_data_miner\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"test\": 123}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello, respond with a JSON: {\\\"test\\\": 123}\", \n",
    "                                  generation_config=genai.types.GenerationConfig(\n",
    "                                      temperature=0.1,\n",
    "                                      max_output_tokens=200\n",
    "                                  ))\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5968f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated JSON schema defined\n"
     ]
    }
   ],
   "source": [
    "VALUATION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"property_id\",\n",
    "        \"report_reference\",\n",
    "        \"market_value_amount\",\n",
    "        \"metadata\"\n",
    "    ],\n",
    "    \"properties\": {\n",
    "\n",
    "        # --- CORE IDENTIFIERS ---\n",
    "        \"property_id\": {\"type\": \"string\"},\n",
    "        \"report_reference\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- TITLE INFORMATION ---\n",
    "        \"title_number\": {\"type\": \"string\"},\n",
    "        \"lr_number\": {\"type\": \"string\"},\n",
    "        \"ir_number\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- CLIENT + VALUER ---\n",
    "        \"client_name\": {\"type\": \"string\"},\n",
    "        \"valuer_name\": {\"type\": \"string\"},\n",
    "        \"inspection_date\": {\"type\": \"string\"},\n",
    "        \"valuation_date\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- LOCATION ---\n",
    "        \"location_county\": {\"type\": \"string\"},\n",
    "        \"location_description\": {\"type\": \"string\"},\n",
    "        \"location_coordinates\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- PARCEL DETAILS ---\n",
    "        \"plot_area_hectares\": {\"type\": \"number\"},\n",
    "        \"plot_area_acres\": {\"type\": \"number\"},\n",
    "        \"land_use\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- NEW LAND PHYSICAL FEATURES ---\n",
    "        \"plot_shape\": {\"type\": \"string\"},     # rectangular, irregular, square\n",
    "        \"soil_type\": {\"type\": \"string\"},      # black cotton, red soil, sandy, loam, murram\n",
    "        \"gradient\": {\"type\": \"string\"},       # flat, gentle slope, moderate slope, steep\n",
    "        \"drainage\": {\"type\": \"string\"},       # optional recommendation\n",
    "        \"vegetation\": {\"type\": \"string\"},     # optional recommendation\n",
    "\n",
    "        # --- TENURE + OWNERSHIP ---\n",
    "        \"tenure_type\": {\"type\": \"string\"},\n",
    "        \"registered_proprietor\": {\"type\": \"string\"},\n",
    "        \"ownership_type\": {\"type\": \"string\"},\n",
    "        \"encumbrances\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- VALUATION ---\n",
    "        \"market_value_amount\": {\"type\": \"number\"},\n",
    "        \"market_value_currency\": {\"type\": \"string\"},\n",
    "\n",
    "        # --- METADATA ---\n",
    "        \"metadata\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"source_file\": {\"type\": \"string\"},\n",
    "                \"file_size_kb\": {\"type\": \"number\"},\n",
    "                \"processing_time_seconds\": {\"type\": \"number\"},\n",
    "                \"ocr_used\": {\"type\": \"boolean\"},\n",
    "                \"pages_processed\": {\"type\": \"number\"},\n",
    "                \"model_name\": {\"type\": \"string\"},\n",
    "                \"timestamp\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Updated JSON schema defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e62af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(data: Dict) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate extracted data against schema\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=VALUATION_SCHEMA)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"âœ“ Validation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee9a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Supabase client initialized\n"
     ]
    }
   ],
   "source": [
    "def init_supabase() -> Optional[Client]:\n",
    "    \"\"\"Initialize Supabase client\"\"\"\n",
    "    try:\n",
    "        if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "            print(\"âš  Supabase credentials not configured\")\n",
    "            return None\n",
    "        \n",
    "        client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        print(\"âœ“ Supabase client initialized\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Supabase init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "supabase_client = init_supabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_TABLE = \"valuations_2025_clean_v2_backup\"   \n",
    "\n",
    "\n",
    "def sanitize_dates_for_supabase(data: dict) -> dict:\n",
    "    \"\"\"Convert empty string dates to None for Supabase insert\"\"\"\n",
    "    date_fields = [\"inspection_date\", \"valuation_date\"]\n",
    "    for field in date_fields:\n",
    "        if field in data and (data[field] == \"\" or data[field] is None):\n",
    "            data[field] = None\n",
    "    return data\n",
    "\n",
    "\n",
    "def upload_to_supabase(client: Client, data: Dict, filename: str) -> bool:\n",
    "    \"\"\"Upload extracted valuation data to Supabase (restricted to valid columns).\"\"\"\n",
    "    if client is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        upload_data = sanitize_dates_for_supabase(data.copy())\n",
    "\n",
    "        # Allowed columns EXACTLY as in your Supabase table\n",
    "        allowed_columns = {\n",
    "            \"property_id\",\n",
    "            \"report_reference\",\n",
    "            \"title_number\",\n",
    "            \"lr_number\",\n",
    "            \"ir_number\",\n",
    "            \"client_name\",\n",
    "            \"valuer_name\",\n",
    "            \"inspection_date\",\n",
    "            \"valuation_date\",\n",
    "            \"location_county\",\n",
    "            \"location_description\",\n",
    "            \"location_coordinates\",\n",
    "            \"plot_area_hectares\",\n",
    "            \"plot_area_acres\",\n",
    "            \"land_use\",\n",
    "            \"plot_shape\",\n",
    "            \"soil_type\",\n",
    "            \"gradient\",\n",
    "            \"drainage\",\n",
    "            \"vegetation\",\n",
    "            \"tenure_type\",\n",
    "            \"registered_proprietor\",\n",
    "            \"ownership_type\",\n",
    "            \"encumbrances\",\n",
    "            \"market_value_amount\",\n",
    "            \"market_value_currency\",\n",
    "            \"metadata\"\n",
    "        }\n",
    "\n",
    "        # Remove any field not in table\n",
    "        clean_record = {k: v for k, v in upload_data.items() if k in allowed_columns}\n",
    "\n",
    "        # Insert into your table\n",
    "        client.table(SUPABASE_TABLE).insert(clean_record).execute()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Supabase upload failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f73ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def process_scanned_pdf_with_ocr(pdf_path: str, dpi: int = 150) -> str:\n",
    "    \"\"\"Convert scanned PDF to text using Tesseract OCR safely (memory-friendly).\"\"\"\n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "        import pytesseract\n",
    "        import gc\n",
    "\n",
    "        # Set Tesseract path\n",
    "        if os.path.exists(TESSERACT_PATH):\n",
    "            pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "        else:\n",
    "            raise Exception(f\"Tesseract not found at {TESSERACT_PATH}. Install from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        print(f\"    â†’ Converting PDF to images at {dpi} dpi...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, fmt='jpeg', thread_count=2)\n",
    "        print(f\"    â†’ Running OCR on {len(images)} pages...\")\n",
    "        \n",
    "        all_text = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"      â†’ Page {i+1}/{len(images)}...\", end=' ')\n",
    "            \n",
    "            # OCR the page (use psm 6 for memory efficiency)\n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                lang='eng',\n",
    "                config='--psm 6'\n",
    "            ).strip()\n",
    "            \n",
    "            if text:\n",
    "                all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "                print(f\"âœ“ {len(text)} chars\")\n",
    "            else:\n",
    "                print(\"(empty)\")\n",
    "            \n",
    "            # Free memory after each page\n",
    "            image.close()\n",
    "            del image\n",
    "            gc.collect()\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        print(f\"    â†’ OCR complete: {len(full_text)} characters\")\n",
    "        \n",
    "        # Save OCR output for debugging\n",
    "        ocr_file = os.path.join(ERROR_LOG_DIR, f\"{Path(pdf_path).stem}_ocr_text.txt\")\n",
    "        with open(ocr_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "print(\"âœ“ Updated OCR function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd186301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF extraction with fallback defined\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_with_fallback(pdf_path: str) -> str:\n",
    "    \"\"\"Try multiple extraction methods\"\"\"\n",
    "    \n",
    "    # Try pdfplumber first\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        print(f\"    â†’ Trying pdfplumber...\")\n",
    "        \n",
    "        all_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        \n",
    "        if len(full_text) > 500:\n",
    "            print(f\"    âœ“ pdfplumber: {len(full_text)} chars\")\n",
    "            return full_text\n",
    "        else:\n",
    "            print(f\"    â†’ Only {len(full_text)} chars, trying OCR...\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â†’ pdfplumber failed, trying OCR...\")\n",
    "    \n",
    "    # Use OCR\n",
    "    return process_scanned_pdf_with_ocr(pdf_path)\n",
    "\n",
    "print(\"âœ“ PDF extraction with fallback defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72030385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def clean_text_for_model(text: str) -> str:\n",
    "    \"\"\"Remove page headers, footers, and empty lines\"\"\"\n",
    "    # Remove page markers like --- Page 3 ---\n",
    "    text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "    # Remove multiple empty lines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_report_reference(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect report reference numbers like:\n",
    "    SOO/DOO/5310/1/25, SOO/5307/1/25, DOO/5307/1/25, VP/1122/24\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b(?:[A-Z]{2,3}/)?(?:[A-Z]{2,3}/)?\\d{3,5}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        # Choose the most complete / longest match\n",
    "        return max(matches, key=len)\n",
    "    return \"\"\n",
    "\n",
    "def clean_gemini_json(response_text: str) -> str:\n",
    "    \"\"\"Extract JSON from Gemini response\"\"\"\n",
    "    if '```json' in response_text:\n",
    "        response_text = response_text.split('```json')[1].split('```')[0]\n",
    "    elif '```' in response_text:\n",
    "        response_text = response_text.split('```')[1].split('```')[0]\n",
    "    response_text = response_text.strip()\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}')\n",
    "    if start != -1 and end != -1:\n",
    "        response_text = response_text[start:end+1]\n",
    "    return response_text\n",
    "\n",
    "def deep_merge(d1, d2):\n",
    "    \"\"\"Merge dictionaries recursively\"\"\"\n",
    "    for k, v in d2.items():\n",
    "        if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
    "            d1[k] = deep_merge(d1[k], v)\n",
    "        else:\n",
    "            d1[k] = v\n",
    "    return d1\n",
    "\n",
    "# -----------------------------\n",
    "# Extraction function\n",
    "# -----------------------------\n",
    "def extract_with_gemini(text_content: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract structured property valuation data from a Kenyan PDF OCR text.\n",
    "    Two-pass approach: first IDs, then land/valuation.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_result = {}\n",
    "    cleaned_text = clean_text_for_model(text_content)\n",
    "    report_ref_guess = find_report_reference(cleaned_text[:3000])  # first 3k chars\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 1: Core identifiers\n",
    "    # -----------------------------\n",
    "    prompt_ids = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- report_reference (like SOO/DOO/5310/1/25)\n",
    "- title_number\n",
    "- lr_number\n",
    "- ir_number\n",
    "- client_name\n",
    "- valuer_name\n",
    "- inspection_date\n",
    "- valuation_date\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as empty string \"\".\n",
    "If multiple report_reference exist, choose the longest one.\n",
    "Use this as a hint for report_reference if present: \"{report_ref_guess}\"\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"property_id\": \"\",\n",
    "  \"report_reference\": \"\",\n",
    "  \"title_number\": \"\",\n",
    "  \"lr_number\": \"\",\n",
    "  \"ir_number\": \"\",\n",
    "  \"client_name\": \"\",\n",
    "  \"valuer_name\": \"\",\n",
    "  \"inspection_date\": \"\",\n",
    "  \"valuation_date\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text[:5000]}\n",
    "\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            prompt_ids,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=2048)\n",
    "        )\n",
    "        ids_json = clean_gemini_json(response.text)\n",
    "        extracted_ids = json.loads(ids_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini ID extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 2: Land, valuation, metadata\n",
    "    # -----------------------------\n",
    "    prompt_land = f\"\"\"\n",
    "Extract the following from this Kenyan valuation report OCR text:\n",
    "\n",
    "- location_county\n",
    "- location_description\n",
    "- location_coordinates\n",
    "- plot_area_hectares\n",
    "- plot_area_acres\n",
    "- land_use\n",
    "- plot_shape\n",
    "- soil_type\n",
    "- gradient\n",
    "- drainage\n",
    "- vegetation\n",
    "- tenure_type\n",
    "- registered_proprietor\n",
    "- ownership_type\n",
    "- encumbrances\n",
    "- market_value_amount\n",
    "- market_value_currency\n",
    "\n",
    "Use only values explicitly found in the text.\n",
    "Return missing fields as \"\" for strings and 0 for numbers.\n",
    "\n",
    "Return ONLY JSON following this schema exactly:\n",
    "{{\n",
    "  \"location_county\": \"\",\n",
    "  \"location_description\": \"\",\n",
    "  \"location_coordinates\": \"\",\n",
    "  \"plot_area_hectares\": 0,\n",
    "  \"plot_area_acres\": 0,\n",
    "  \"land_use\": \"\",\n",
    "  \"plot_shape\": \"\",\n",
    "  \"soil_type\": \"\",\n",
    "  \"gradient\": \"\",\n",
    "  \"drainage\": \"\",\n",
    "  \"vegetation\": \"\",\n",
    "  \"tenure_type\": \"\",\n",
    "  \"registered_proprietor\": \"\",\n",
    "  \"ownership_type\": \"\",\n",
    "  \"encumbrances\": \"\",\n",
    "  \"market_value_amount\": 0,\n",
    "  \"market_value_currency\": \"\"\n",
    "}}\n",
    "\n",
    "TEXT:\n",
    "{cleaned_text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt_land,\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4096)\n",
    "        )\n",
    "        land_json = clean_gemini_json(response.text)\n",
    "        extracted_land = json.loads(land_json)\n",
    "        combined_result = deep_merge(combined_result, extracted_land)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Gemini Land extraction failed: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final adjustments\n",
    "    # -----------------------------\n",
    "    if not combined_result.get(\"property_id\"):\n",
    "        combined_result[\"property_id\"] = Path(filename).stem\n",
    "    if \"report_reference\" not in combined_result or not combined_result[\"report_reference\"]:\n",
    "        combined_result[\"report_reference\"] = report_ref_guess\n",
    "\n",
    "    # Metadata skeleton (to be populated later)\n",
    "    if \"metadata\" not in combined_result:\n",
    "        combined_result[\"metadata\"] = {\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": 0,\n",
    "            \"processing_time_seconds\": 0,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": 0,\n",
    "            \"model_name\": \"gemini-2.5-flash\",\n",
    "            \"timestamp\": \"\"\n",
    "        }\n",
    "    # Ensure ID/date fields stay as strings\n",
    "    string_fields = [\n",
    "        \"title_number\",\n",
    "        \"lr_number\",\n",
    "        \"ir_number\",\n",
    "        \"client_name\",\n",
    "        \"valuer_name\",\n",
    "        \"inspection_date\",\n",
    "        \"valuation_date\",\n",
    "    ]\n",
    "\n",
    "    for field in string_fields:\n",
    "        if field in combined_result and combined_result[field] is None:\n",
    "            combined_result[field] = \"\"\n",
    "\n",
    "\n",
    "    return combined_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f11142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete pipeline defined with full metadata\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf_complete(pdf_path: str, filename: str) -> dict:\n",
    "    \"\"\"Complete pipeline: OCR â†’ Gemini â†’ Validate â†’ Save\"\"\"\n",
    "    result = {\n",
    "        'filename': filename,\n",
    "        'success': False,\n",
    "        'stage': None,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: OCR\n",
    "        result['stage'] = 'ocr'\n",
    "        print(f\"\\n  ðŸ“„ Stage 1: OCR\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        text_content = process_pdf_with_fallback(pdf_path)\n",
    "        pages_processed = text_content.count('--- Page')  # approximate page count\n",
    "        result['timing']['ocr'] = time.time() - stage_start\n",
    "        \n",
    "        if len(text_content) < 200:\n",
    "            raise Exception(f\"Insufficient text: {len(text_content)} chars\")\n",
    "        \n",
    "        print(f\"  âœ“ {len(text_content)} chars in {result['timing']['ocr']:.1f}s\")\n",
    "        \n",
    "        # Stage 2: Gemini\n",
    "        result['stage'] = 'gemini'\n",
    "        print(f\"\\n  ðŸ¤– Stage 2: Gemini\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        extracted_data = extract_with_gemini(text_content, filename)\n",
    "        result['timing']['gemini'] = time.time() - stage_start\n",
    "        result['data'] = extracted_data\n",
    "        \n",
    "        print(f\"  âœ“ Extracted in {result['timing']['gemini']:.1f}s\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Inject real metadata\n",
    "        # -------------------------\n",
    "        file_size_kb = round(Path(pdf_path).stat().st_size / 1024, 2)\n",
    "        processing_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        if \"metadata\" not in extracted_data:\n",
    "            extracted_data[\"metadata\"] = {}\n",
    "        \n",
    "        extracted_data[\"metadata\"].update({\n",
    "            \"source_file\": filename,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Validation\n",
    "        result['stage'] = 'validation'\n",
    "        print(f\"\\n  âœ… Stage 3: Validation\")\n",
    "        \n",
    "        is_valid, validation_error = validate_extracted_data(extracted_data)\n",
    "        if not is_valid:\n",
    "            print(f\"  âš  Warning: {validation_error[:100]}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Valid\")\n",
    "        \n",
    "        # Stage 4: Save\n",
    "        result['stage'] = 'saving'\n",
    "        print(f\"\\n  ðŸ’¾ Stage 4: Save\")\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{Path(filename).stem}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  âœ“ Saved to {Path(output_file).name}\")\n",
    "        \n",
    "        # Stage 5: Supabase\n",
    "        if supabase_client and is_valid:\n",
    "            result['stage'] = 'supabase'\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted_data, filename)\n",
    "                print(f\"  âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Supabase: {str(e)[:50]}\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['timing']['total'] = time.time() - start_time\n",
    "        \n",
    "        error_file = os.path.join(ERROR_LOG_DIR, f\"{Path(filename).stem}_error.txt\")\n",
    "        with open(error_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Stage: {result['stage']}\\nError: {result['error']}\\n\")\n",
    "        \n",
    "        print(f\"\\n  âœ— Failed: {result['error'][:100]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Complete pipeline defined with full metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2157cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_complete():\n",
    "    \"\"\"Process all PDFs in PDF_DIR, generate OCR, send to Gemini, save JSON, and upload to Supabase.\n",
    "       Skips PDFs that already have a JSON output file.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Scan directories\n",
    "    # -------------------------\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDFs found in {PDF_DIR}\")\n",
    "        return []\n",
    "\n",
    "    # Identify JSON files already processed\n",
    "    processed_json_stems = {p.stem for p in Path(OUTPUT_DIR).glob(\"*.json\")}\n",
    "\n",
    "    # Filter: process ONLY new PDFs\n",
    "    pdf_files = [p for p in pdf_files if p.stem not in processed_json_stems]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"ðŸŽ‰ All PDFs already processed â€” nothing to do.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ PROCESSING {len(pdf_files)} NEW PDFs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "    # Process each PDF\n",
    "    # -------------------------\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        file_start_time = time.time()\n",
    "        file_size_kb = round(pdf_path.stat().st_size / 1024, 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. OCR extraction\n",
    "        # -------------------------\n",
    "        text_content = process_pdf_with_fallback(str(pdf_path))\n",
    "        pages_processed = text_content.count('--- Page')  # reliable heuristic\n",
    "        print(f\"  âœ“ OCR complete ({pages_processed} pages, {len(text_content)} chars)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Gemini extraction\n",
    "        # -------------------------\n",
    "        extracted = extract_with_gemini(text_content, pdf_path.name)\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Add metadata to JSON\n",
    "        # -------------------------\n",
    "        processing_time = round(time.time() - file_start_time, 2)\n",
    "        if \"metadata\" not in extracted:\n",
    "            extracted[\"metadata\"] = {}\n",
    "\n",
    "        extracted[\"metadata\"].update({\n",
    "            \"source_file\": pdf_path.name,\n",
    "            \"file_size_kb\": file_size_kb,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ocr_used\": True,\n",
    "            \"pages_processed\": pages_processed,\n",
    "            \"model_name\": GEMINI_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Save the JSON output\n",
    "        # -------------------------\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{pdf_path.stem}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ“ Saved â†’ {output_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Upload to Supabase\n",
    "        # -------------------------\n",
    "        if supabase_client:\n",
    "            try:\n",
    "                upload_to_supabase(supabase_client, extracted, pdf_path.name)\n",
    "                print(f\"âœ“ Uploaded to Supabase\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Supabase upload failed: {str(e)[:120]}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Track results\n",
    "        # -------------------------\n",
    "        results.append({\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"success\": bool(extracted),\n",
    "            \"data\": extracted,\n",
    "            \"timing\": {\"processing_seconds\": processing_time},\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Save batch summary\n",
    "    # -------------------------\n",
    "    total_time = round(time.time() - total_start, 2)\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": GEMINI_MODEL,\n",
    "            \"total_pdfs\": len(pdf_files),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"results\": results,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Batch finished in {total_time/60:.1f} minutes\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e1bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª TESTING: LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY.pdf\n",
      "\n",
      "\n",
      "  ðŸ“„ Stage 1: OCR\n",
      "    â†’ Trying pdfplumber...\n",
      "    â†’ Only 0 chars, trying OCR...\n",
      "    â†’ Converting PDF to images at 150 dpi...\n",
      "    â†’ Running OCR on 22 pages...\n",
      "      â†’ Page 1/22... âœ“ 1151 chars\n",
      "      â†’ Page 2/22... âœ“ 2401 chars\n",
      "      â†’ Page 3/22... âœ“ 273 chars\n",
      "      â†’ Page 4/22... âœ“ 178 chars\n",
      "      â†’ Page 5/22... âœ“ 884 chars\n",
      "      â†’ Page 6/22... âœ“ 1435 chars\n",
      "      â†’ Page 7/22... âœ“ 1399 chars\n",
      "      â†’ Page 8/22... âœ“ 1023 chars\n",
      "      â†’ Page 9/22... âœ“ 1961 chars\n",
      "      â†’ Page 10/22... âœ“ 1901 chars\n",
      "      â†’ Page 11/22... âœ“ 1533 chars\n",
      "      â†’ Page 12/22... âœ“ 735 chars\n",
      "      â†’ Page 13/22... âœ“ 1491 chars\n",
      "      â†’ Page 14/22... âœ“ 557 chars\n",
      "      â†’ Page 15/22... âœ“ 958 chars\n",
      "      â†’ Page 16/22... âœ“ 790 chars\n",
      "      â†’ Page 17/22... (empty)\n",
      "      â†’ Page 18/22... âœ“ 11 chars\n",
      "      â†’ Page 19/22... (empty)\n",
      "      â†’ Page 20/22... (empty)\n",
      "      â†’ Page 21/22... âœ“ 18 chars\n",
      "      â†’ Page 22/22... âœ“ 44 chars\n",
      "    â†’ OCR complete: 19074 characters\n",
      "  âœ“ 19074 chars in 68.9s\n",
      "\n",
      "  ðŸ¤– Stage 2: Gemini\n",
      "  âœ“ Extracted in 21.2s\n",
      "\n",
      "  âœ… Stage 3: Validation\n",
      "  âœ“ Valid\n",
      "\n",
      "  ðŸ’¾ Stage 4: Save\n",
      "  âœ“ Saved to LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY.json\n",
      "  âš  Supabase: Supabase upload failed: {'message': \"Could not fin\n",
      "\n",
      "âœ… TEST PASSED!\n",
      "\n",
      "Preview:\n",
      "{\n",
      "  \"property_id\": \"LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY\",\n",
      "  \"report_reference\": \"SOO/DOO/5390/1/25\",\n",
      "  \"title_number\": \"\",\n",
      "  \"lr_number\": \"27903/29 (N 83 FOLIO 64 FILE 25310)\",\n",
      "  \"ir_number\": \"\",\n",
      "  \"client_name\": \"The Co-operative Bank of Kenya Limited\",\n",
      "  \"valuer_name\": \"NW Realite Limited\",\n",
      "  \"inspection_date\": \"\",\n",
      "  \"valuation_date\": \"\",\n",
      "  \"location_county\": \"Nairobi County\",\n",
      "  \"location_description\": \"The property is located within Pipeline Ward within Embakasi South Constituency in Nairobi County. It lies adjacent to The Home Point Lounge, approximately 900 meters to the Northeast of Southfield Mall and 165 meters to the Northeast of Kenya Builders and Conctete Company Limited within Pipeline Area of Embakasi in Nairobi County.\",\n",
      "  \"location_coordinates\": \"1\\u00b019'14.7\\\"S 36\\u00b053'35.5\\\"E\",\n",
      "  \"plot_area_hectares\": 0.0287,\n",
      "  \"plot_area_acres\": 0.0709,\n",
      "  \"land_use\": \"Residential Property\",\n",
      "  \"plot_shape\": \"rectangular\",\n",
      "  \"soil_type\": \"mixed soils\",\n",
      "  \"gradient\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': 'LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY.pdf',\n",
       " 'success': True,\n",
       " 'stage': 'supabase',\n",
       " 'error': None,\n",
       " 'data': {'property_id': 'LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY',\n",
       "  'report_reference': 'SOO/DOO/5390/1/25',\n",
       "  'title_number': '',\n",
       "  'lr_number': '27903/29 (N 83 FOLIO 64 FILE 25310)',\n",
       "  'ir_number': '',\n",
       "  'client_name': 'The Co-operative Bank of Kenya Limited',\n",
       "  'valuer_name': 'NW Realite Limited',\n",
       "  'inspection_date': '',\n",
       "  'valuation_date': '',\n",
       "  'location_county': 'Nairobi County',\n",
       "  'location_description': 'The property is located within Pipeline Ward within Embakasi South Constituency in Nairobi County. It lies adjacent to The Home Point Lounge, approximately 900 meters to the Northeast of Southfield Mall and 165 meters to the Northeast of Kenya Builders and Conctete Company Limited within Pipeline Area of Embakasi in Nairobi County.',\n",
       "  'location_coordinates': '1Â°19\\'14.7\"S 36Â°53\\'35.5\"E',\n",
       "  'plot_area_hectares': 0.0287,\n",
       "  'plot_area_acres': 0.0709,\n",
       "  'land_use': 'Residential Property',\n",
       "  'plot_shape': 'rectangular',\n",
       "  'soil_type': 'mixed soils',\n",
       "  'gradient': 'gentle',\n",
       "  'drainage': '',\n",
       "  'vegetation': '',\n",
       "  'tenure_type': 'Freehold',\n",
       "  'registered_proprietor': 'Catherine Benard Chiuri',\n",
       "  'ownership_type': '',\n",
       "  'encumbrances': 'Charge dated 3rd April 2018 in favour of Equity Bank (Kenya) Limited to secure a sum of KShs. 13,300,000.00.',\n",
       "  'market_value_amount': 60000000.0,\n",
       "  'market_value_currency': 'KShs.',\n",
       "  'metadata': {'source_file': 'LR NO 27903-29 PIPELINE AREA EMBAKASI NAIROBI COUNTY.pdf',\n",
       "   'file_size_kb': 11674.02,\n",
       "   'processing_time_seconds': 90.12,\n",
       "   'ocr_used': True,\n",
       "   'pages_processed': 19,\n",
       "   'model_name': 'models/gemini-2.5-flash',\n",
       "   'timestamp': '2025-12-02T12:05:06.993620'}},\n",
       " 'timing': {'ocr': 68.85298895835876,\n",
       "  'gemini': 21.217646837234497,\n",
       "  'total': 95.83046960830688}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_single_pdf():\n",
    "    \"\"\"Test with first PDF\"\"\"\n",
    "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    \n",
    "    test_file = pdf_files[0]\n",
    "    print(f\"\\nðŸ§ª TESTING: {test_file.name}\\n\")\n",
    "    \n",
    "    result = process_single_pdf_complete(str(test_file), test_file.name)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… TEST PASSED!\")\n",
    "        print(f\"\\nPreview:\")\n",
    "        print(json.dumps(result['data'], indent=2)[:1000])\n",
    "    else:\n",
    "        print(f\"\\nâŒ TEST FAILED\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "test_single_pdf()\n",
    "# print(\"or:  results = process_all_pdfs_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_all_pdfs_complete()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
